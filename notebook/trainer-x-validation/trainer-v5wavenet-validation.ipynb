{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainver v5 code validation\n",
    "\n",
    "Simple minimal training runs, to validate v5 code\n",
    "\n",
    "> Important note: These example focuses only on how to configure your dataset, and does not properly perform checkmarking - for trainer configurations refer to the training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/trainer-x-validation\n",
      "INFERENCE_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"trainer-v5wavenet-validation L6-D512\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5wavenet/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5wavenet/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial setup\n",
    "\n",
    "Before we go into the dataset setup, lets perform an initial setup for all the folders we need, and a small toy model which we would use throughout the various examples within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:17:53,853] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-v5wavenet-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Setup the folders we will need\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "\n",
    "# Initialized a simple L6-D512 model\n",
    "!cd \"{TRAINER_DIR}\" && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size neox --skip-if-exists ../model/L6-D512-v5wavenet-neox-init.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick train for v5\n",
    "\n",
    "Preload and train the mini-v5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:21:15,553] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 796.34it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-59b52cf35738a88f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5eb10bc9123e7e5f_*_of_00016.arrow\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py ../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:39:06,328] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5wavenet-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5wavenet-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3872385575\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3872385575\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 732.76it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-59b52cf35738a88f_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5eb10bc9123e7e5f_*_of_00016.arrow\n",
      "[rank: 0] Global seed set to 3872385575                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-07 13:39:11,474] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.05765652656555176 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1] and sizes[(71966720, False), (96, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0:  14%|▏| 182/1323 [00:43<04:30,  4.22it/s, v_num=yxvt, train/loss=9.810]"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-x-validation/mini-v5wavenet-enwiki.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (full, train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-07 13:38:42,534] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet/export_checkpoint.py\", line 623, in <module>\n",
      "    convert_zero_checkpoint_to_fp32_state_dict(args.checkpoint_dir, output_file)\n",
      "  File \"/home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet/export_checkpoint.py\", line 537, in convert_zero_checkpoint_to_fp32_state_dict\n",
      "    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v5wavenet/export_checkpoint.py\", line 516, in get_fp32_state_dict_from_zero_checkpoint\n",
      "    raise ValueError(f\"Unable to find 'latest' file at {latest_path}\")\n",
      "ValueError: Unable to find 'latest' file at ../checkpoint/trainer-validation/mini-v5wavenet-enwiki/last.ckpt/latest\n"
     ]
    }
   ],
   "source": [
    "# Lets convert the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 export_checkpoint.py \\\n",
    "        ../checkpoint/trainer-validation/mini-v5wavenet-enwiki/last.ckpt/ \\\n",
    "        ../model/mini-v5wavenet-enwiki.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And run the dragon test\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 dragon_test.py ../model/mini-v5wavenet-enwiki.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
