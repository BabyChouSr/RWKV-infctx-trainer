{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainver v5 code validation\n",
    "\n",
    "Simple minimal training runs, to validate v5 code\n",
    "\n",
    "> Important note: These example focuses only on how to configure your dataset, and does not properly perform checkmarking - for trainer configurations refer to the training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: False\n",
      "GPU_DEVICES: auto\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=False\n",
    "WANDB_PREFIX=\"trainer-v5-validation L6-D512\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial setup\n",
    "\n",
    "Before we go into the dataset setup, lets perform an initial setup for all the folders we need, and a small toy model which we would use throughout the various examples within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-05 09:30:06,188] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-v5-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "50277 512   -0.1 emb.weight\n",
      "512   512   0    blocks.0.att.receptance.weight\n",
      "512   512   0    blocks.0.att.key.weight\n",
      "512   512   1.0  blocks.0.att.value.weight\n",
      "512   512   0    blocks.0.att.output.weight\n",
      "2048  512   1.0  blocks.0.ffn.key.weight\n",
      "512   512   0    blocks.0.ffn.receptance.weight\n",
      "512   2048  0    blocks.0.ffn.value.weight\n",
      "512   512   0    blocks.1.att.receptance.weight\n",
      "512   512   0    blocks.1.att.key.weight\n",
      "512   512   1.0  blocks.1.att.value.weight\n",
      "512   512   0    blocks.1.att.output.weight\n",
      "2048  512   1.0  blocks.1.ffn.key.weight\n",
      "512   512   0    blocks.1.ffn.receptance.weight\n",
      "512   2048  0    blocks.1.ffn.value.weight\n",
      "512   512   0    blocks.2.att.receptance.weight\n",
      "512   512   0    blocks.2.att.key.weight\n",
      "512   512   1.0  blocks.2.att.value.weight\n",
      "512   512   0    blocks.2.att.output.weight\n",
      "2048  512   1.0  blocks.2.ffn.key.weight\n",
      "512   512   0    blocks.2.ffn.receptance.weight\n",
      "512   2048  0    blocks.2.ffn.value.weight\n",
      "512   512   0    blocks.3.att.receptance.weight\n",
      "512   512   0    blocks.3.att.key.weight\n",
      "512   512   1.0  blocks.3.att.value.weight\n",
      "512   512   0    blocks.3.att.output.weight\n",
      "2048  512   1.0  blocks.3.ffn.key.weight\n",
      "512   512   0    blocks.3.ffn.receptance.weight\n",
      "512   2048  0    blocks.3.ffn.value.weight\n",
      "512   512   0    blocks.4.att.receptance.weight\n",
      "512   512   0    blocks.4.att.key.weight\n",
      "512   512   1.0  blocks.4.att.value.weight\n",
      "512   512   0    blocks.4.att.output.weight\n",
      "2048  512   1.0  blocks.4.ffn.key.weight\n",
      "512   512   0    blocks.4.ffn.receptance.weight\n",
      "512   2048  0    blocks.4.ffn.value.weight\n",
      "512   512   0    blocks.5.att.receptance.weight\n",
      "512   512   0    blocks.5.att.key.weight\n",
      "512   512   1.0  blocks.5.att.value.weight\n",
      "512   512   0    blocks.5.att.output.weight\n",
      "2048  512   1.0  blocks.5.ffn.key.weight\n",
      "512   512   0    blocks.5.ffn.receptance.weight\n",
      "512   2048  0    blocks.5.ffn.value.weight\n",
      "50277 512   0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Setup the folders we will need\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "\n",
    "# Initialized a simple L6-D512 model\n",
    "!cd ../../RWKV-v5/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size neox --skip-if-exists ../model/L6-D512-v5-neox-init.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick train for v5\n",
    "\n",
    "Preload and train the mini-v5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 945.30it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    python3 preload_datapath.py ../notebook/trainer-x-validation/mini-v5-enwiki.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-05 10:59:06,695] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-x-validation/mini-v5-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'], args=['fit', '-c', '../notebook/trainer-x-validation/mini-v5-enwiki.yaml', '--trainer.logger.init_args.name=trainer-v5-validation L6-D512 (full, train-ctx=4096, data-ctx=4096, deepspeed_stage_1)', '--trainer.strategy=deepspeed_stage_1', '--trainer.devices=auto'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 51716742\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 51716742\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 964.43it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e6926d59afde2d0b_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-f11aefa46fc8bf2e_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-2d7d24ac9893a2fa_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 51716742                                           \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-05 10:59:09,878] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/ubuntu/picocreator-memory-experiment/checkpoint/trainer-validation/mini-v5-enwiki exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.06029486656188965 seconds\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/adam/fused_adam.py:96: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1688627653114/work/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71966720, False), (48, False), (48, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 1315/1315 [06:13<00:00,  3.52it/s, v_num=haho, train/loss=7.840\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▎                 | 1/14 [00:00<00:03,  3.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|██▋                | 2/14 [00:00<00:02,  4.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|████               | 3/14 [00:00<00:02,  4.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|█████▍             | 4/14 [00:00<00:02,  4.35it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|██████▊            | 5/14 [00:01<00:02,  4.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|████████▏          | 6/14 [00:01<00:01,  4.43it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 7/14 [00:01<00:01,  4.46it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▊        | 8/14 [00:01<00:01,  4.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|████████████▏      | 9/14 [00:02<00:01,  4.49it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|████████████▊     | 10/14 [00:02<00:00,  4.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|██████████████▏   | 11/14 [00:02<00:00,  4.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%|███████████████▍  | 12/14 [00:02<00:00,  4.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 13/14 [00:02<00:00,  4.53it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 1315/1315 [06:19<00:00,  3.47it/s, v_num=haho, train/loss=7.840\u001b[A\n",
      "Epoch 0: 100%|█| 1315/1315 [06:19<00:00,  3.47it/s, v_num=haho, train/loss=7.840/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1825: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 1315/1315 [06:20<00:00,  3.45it/s, v_num=haho, train/loss=7.840\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 lightning_trainer.py fit \\\n",
    "        -c ../notebook/trainer-x-validation/mini-v5-enwiki.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (full, train-ctx=4096, data-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-05 11:18:01,356] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/trainer-validation/mini-v5-enwiki/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 1\n",
      "Parsing checkpoint created by deepspeed==0.9.5\n",
      "Reconstructed fp32 state dict with 126 params 71966816 elements\n",
      "Saving fp32 state dict to ../model/mini-v5-enwiki.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v5 && \\\n",
    "    python3 export_checkpoint.py \\\n",
    "        ../checkpoint/trainer-validation/mini-v5-enwiki/last.ckpt/ \\\n",
    "        ../model/mini-v5-enwiki.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
