{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2 & 3 Validation\n",
    "This model being trained has the same settings as raven 1B5 model.\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "The goal is to validate the trainer across deepspeed 2 & 3 - with and without offload. All other training params remain constant.\n",
    "\n",
    "Note, you will need a dual+ GPU setup, that is capable of handling deepspeed 2 (minimum 24GB * 2)\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    ">\n",
    "> And that you have completed the `baseline-setup.ipynb` and the environment setup found in project `README.md`\n",
    "\n",
    "## What does deepspeed 2 & 3 do (With/Without CPU offload) ??\n",
    "\n",
    "Instead of simply splitting the dataset being trained, and having a full copy of nearly everything in all GPU's (aka DDP / DeepSpeed 1).\n",
    "\n",
    "Deepspeed 2, keeps a full copy of the model weights on each GPU, but splits the training gradient descent memory usage into multiple GPUs, or offload it into CPU memory (+ CPU offload option).\n",
    "\n",
    "Deepspeed 3, takes it a step further, and distributes the model weights across all the GPUs, drastically lowering the vram requirement, while increasing the amount of GPU to GPU traffic drastically. Gradient descent memory is still split across multiple GPUs, with the option to offload into CPU memory (Same as deepspeed 2)\n",
    "\n",
    "Finally, Deepspeed 3, also introduce options to further offload such model weights / gradient descent, more into CPU memory or NVMe. However this option was not enabled or explored in the following benchmarks.\n",
    "\n",
    "See more here: https://huggingface.co/docs/transformers/main_classes/deepspeed\n",
    "\n",
    "## Benchmark results on 2 x A5000 (24gb, NvLink, via vast.ai)\n",
    "\n",
    "(benchmark was done on 10th July 2023. With Torch 2.0.1, Cuda 11.8.)\n",
    "\n",
    "| Deepspeed Strat       | Time                  | VRAM Usage       | RAM Usage | Validation Loss |\n",
    "| --------------------- | --------------------- | ---------------- | --------- | --------------- |\n",
    "| Stage 2               | 24 mins : 55 sec      | ~22.3 + 23.8 GB  | ~85 GB    | 6.173           |\n",
    "| Stage 3               | 29 mins : 12 sec      | ~23.0 + 23.2 GB^ | ~85 GB    | 5.665           |\n",
    "| Stage 2 + CPU offload | 43 mins : 08 sec      | ~9.7 + 10.3 GB   | ~128 GB   | 6.124           |\n",
    "| Stage 3 + CPU offload | 1hr : 42mins : 38 sec | ~7.0 + 7.3 GB    | ~145 GB   | 5.668           |\n",
    "\n",
    "---\n",
    "\n",
    "> ^ note in theory deepspeed 3 uses less vram then deepspeed 2, however it will also try to use up more ram then its needed for \"cache\" items if possible, maxing out to the same level as deepspeed 2 here\n",
    "\n",
    "> Torch.JIT was enabled for deepspeed 2, But was disabled for deepspeed 3 (not compatible). Torch.compile was disabled\n",
    "\n",
    "> Full report with charts and figures can be found at WANDB : https://wandb.ai/picocreator/RWKV-InfCtx-Validation/reports/-RWKV-infctx-DeepSpeed-2-3-comparisons--Vmlldzo0ODM5MjAy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and apply your preferred settings\n",
    "\n",
    "Adjust your desired deepspeed settings, and gpu device count.\n",
    "\n",
    "Enable/Disable WANDB here as well ( Enabled by default, as we need the loss curve for this experiment )\n",
    "\n",
    "( note you will need to rerun this cell, if you restart your env )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n"
     ]
    }
   ],
   "source": [
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"infctx-deepspeed\"\n",
    "\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230709_163612-9qq27v2z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed (deepspeed_stage_2, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/9qq27v2z\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 596.54it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-161be9e5a7a54d76_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-45459c41c653f279_*_of_00128.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-09 16:36:32,126] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-09 16:36:48,378] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-09 16:36:48,384] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0640115737915039 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10116028785705566 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06395459175109863 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10399532318115234 seconds\n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0004949569702148438 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0009634494781494141 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [24:55<00:00,  1.76it/s, v_num=7v2z, train/loss=6.120\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:05,  4.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:00<00:05,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:00<00:04,  5.21it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:00<00:04,  5.39it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:00<00:03,  5.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:01<00:03,  5.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:01<00:03,  5.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:01<00:03,  5.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:01<00:03,  5.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:01<00:02,  5.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:01<00:02,  5.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:02<00:02,  5.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:02<00:02,  5.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:02<00:02,  5.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:02<00:02,  5.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:02<00:01,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:02<00:01,  5.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:03<00:01,  5.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:03<00:01,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:03<00:01,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:03<00:01,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:03<00:00,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:03<00:00,  5.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:04<00:00,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:04<00:00,  5.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:04<00:00,  5.89it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [25:14<00:00,  1.74it/s, v_num=7v2z, train/loss=6.120\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [25:14<00:00,  1.74it/s, v_num=7v2z, train/loss=6.120`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [25:14<00:00,  1.74it/s, v_num=7v2z, train/loss=6.120\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 6.21875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 263\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 6.17303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed (deepspeed_stage_2, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/9qq27v2z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230709_163612-9qq27v2z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 new_train.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 2 + Offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230709_095225-1uwkabm4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed (deepspeed_stage_2_offload, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/1uwkabm4\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 635.40it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-161be9e5a7a54d76_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-45459c41c653f279_*_of_00128.arrow\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-09 09:52:44,411] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-09 09:53:00,884] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-09 09:53:00,892] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.351978063583374 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4068613052368164 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06754732131958008 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10294580459594727 seconds\n",
      "Rank: 1 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Rank: 0 partition count [2, 2, 2] and sizes[(757504000, False), (24576, False), (24576, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007128715515136719 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006597042083740234 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [43:08<00:00,  1.02it/s, v_num=abm4, train/loss=6.060\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:06,  4.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:00<00:05,  4.38it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:00<00:05,  4.47it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:00<00:04,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:01<00:04,  4.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:01<00:04,  4.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:01<00:04,  5.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:01<00:03,  5.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:01<00:03,  5.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:01<00:03,  5.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:02<00:03,  5.29it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:02<00:02,  5.34it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:02<00:02,  5.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:02<00:02,  5.45it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:02<00:02,  5.50it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:02<00:01,  5.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:03<00:01,  5.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:03<00:01,  5.53it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:03<00:01,  5.56it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:03<00:01,  5.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:03<00:01,  5.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:03<00:00,  5.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:04<00:00,  5.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:04<00:00,  5.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:04<00:00,  5.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:04<00:00,  5.75it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [44:11<00:00,  1.01s/it, v_num=abm4, train/loss=6.060\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [44:11<00:00,  1.01s/it, v_num=abm4, train/loss=6.060`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [44:11<00:00,  1.01s/it, v_num=abm4, train/loss=6.060\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: - 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 6.15625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 263\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 6.12442\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed (deepspeed_stage_2_offload, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/1uwkabm4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230709_095225-1uwkabm4/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 new_train.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_2_offload, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_2_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230709_132545-ty2zt78b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed (deepspeed_stage_3, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/ty2zt78b\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 556.20it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-161be9e5a7a54d76_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-45459c41c653f279_*_of_00128.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-09 13:26:05,211] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-09 13:26:21,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-09 13:26:21,392] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.07519984245300293 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10122156143188477 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06717443466186523 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06544661521911621 seconds\n",
      "Parameter Offload: Total persistent parameters: 548864 in 268 params\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005288124084472656 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0006351470947265625 seconds\n",
      "\n",
      "  | Name   | Type       | Params | Params per Device\n",
      "----------------------------------------------------------\n",
      "0 | emb    | Embedding  | 102 M  | 51.5 M           \n",
      "1 | blocks | ModuleList | 1.3 B  | 654 M            \n",
      "2 | ln_out | LayerNorm  | 4.1 K  | 2.0 K            \n",
      "3 | head   | Linear     | 102 M  | 51.5 M           \n",
      "----------------------------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/2632 [00:00<?, ?it/s][2023-07-09 13:27:16,384] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'src.model.BlockState'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.\n",
      "Epoch 0:   1%|  | 19/2632 [00:24<56:52,  1.31s/it, v_num=t78b, train/loss=9.620][2023-07-09 13:27:34,232] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   3%|  | 69/2632 [00:59<36:41,  1.16it/s, v_num=t78b, train/loss=8.620][2023-07-09 13:28:08,658] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   3%|  | 79/2632 [01:06<35:40,  1.19it/s, v_num=t78b, train/loss=8.620][2023-07-09 13:28:15,525] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   4%| | 109/2632 [01:27<33:54,  1.24it/s, v_num=t78b, train/loss=9.190][2023-07-09 13:28:37,133] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   5%| | 119/2632 [01:35<33:42,  1.24it/s, v_num=t78b, train/loss=7.910][2023-07-09 13:28:45,512] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   5%| | 129/2632 [01:44<33:43,  1.24it/s, v_num=t78b, train/loss=8.440][2023-07-09 13:28:53,742] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   6%| | 159/2632 [02:07<33:01,  1.25it/s, v_num=t78b, train/loss=8.060][2023-07-09 13:29:17,175] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:   9%| | 239/2632 [03:02<30:26,  1.31it/s, v_num=t78b, train/loss=7.840][2023-07-09 13:30:11,764] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  11%| | 279/2632 [03:29<29:26,  1.33it/s, v_num=t78b, train/loss=7.810][2023-07-09 13:30:38,790] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  12%| | 319/2632 [03:56<28:35,  1.35it/s, v_num=t78b, train/loss=7.690][2023-07-09 13:31:05,882] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  12%|‚ñè| 329/2632 [04:03<28:24,  1.35it/s, v_num=t78b, train/loss=7.660][2023-07-09 13:31:12,817] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  15%|‚ñè| 399/2632 [04:49<27:00,  1.38it/s, v_num=t78b, train/loss=7.340][2023-07-09 13:31:58,899] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  16%|‚ñè| 419/2632 [05:03<26:42,  1.38it/s, v_num=t78b, train/loss=7.720][2023-07-09 13:32:12,770] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  16%|‚ñè| 429/2632 [05:10<26:34,  1.38it/s, v_num=t78b, train/loss=7.590][2023-07-09 13:32:19,846] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  17%|‚ñè| 449/2632 [05:23<26:14,  1.39it/s, v_num=t78b, train/loss=7.780][2023-07-09 13:32:33,214] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  19%|‚ñè| 489/2632 [05:50<25:36,  1.39it/s, v_num=t78b, train/loss=7.160][2023-07-09 13:33:00,078] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  19%|‚ñè| 509/2632 [06:04<25:19,  1.40it/s, v_num=t78b, train/loss=7.470][2023-07-09 13:33:13,465] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  20%|‚ñè| 529/2632 [06:17<25:00,  1.40it/s, v_num=t78b, train/loss=7.530][2023-07-09 13:33:27,055] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  22%|‚ñè| 579/2632 [06:51<24:17,  1.41it/s, v_num=t78b, train/loss=7.410][2023-07-09 13:34:00,386] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  22%|‚ñè| 589/2632 [06:58<24:11,  1.41it/s, v_num=t78b, train/loss=7.250][2023-07-09 13:34:07,765] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  25%|‚ñè| 649/2632 [07:38<23:20,  1.42it/s, v_num=t78b, train/loss=7.000][2023-07-09 13:34:47,598] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  25%|‚ñé| 659/2632 [07:45<23:13,  1.42it/s, v_num=t78b, train/loss=6.750][2023-07-09 13:34:54,904] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  25%|‚ñé| 669/2632 [07:52<23:06,  1.42it/s, v_num=t78b, train/loss=7.000][2023-07-09 13:35:01,692] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  27%|‚ñé| 699/2632 [08:11<22:39,  1.42it/s, v_num=t78b, train/loss=7.410][2023-07-09 13:35:20,950] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  27%|‚ñé| 709/2632 [08:18<22:31,  1.42it/s, v_num=t78b, train/loss=6.660][2023-07-09 13:35:27,584] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  28%|‚ñé| 739/2632 [08:38<22:07,  1.43it/s, v_num=t78b, train/loss=6.970][2023-07-09 13:35:47,631] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  30%|‚ñé| 779/2632 [09:05<21:38,  1.43it/s, v_num=t78b, train/loss=6.910][2023-07-09 13:36:15,197] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  31%|‚ñé| 809/2632 [09:25<21:14,  1.43it/s, v_num=t78b, train/loss=6.970][2023-07-09 13:36:34,921] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  31%|‚ñé| 819/2632 [09:32<21:07,  1.43it/s, v_num=t78b, train/loss=6.660][2023-07-09 13:36:41,969] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  31%|‚ñé| 829/2632 [09:39<21:00,  1.43it/s, v_num=t78b, train/loss=6.720][2023-07-09 13:36:48,950] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  32%|‚ñé| 839/2632 [09:47<20:55,  1.43it/s, v_num=t78b, train/loss=6.880][2023-07-09 13:36:56,888] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  32%|‚ñé| 849/2632 [09:54<20:48,  1.43it/s, v_num=t78b, train/loss=6.750][2023-07-09 13:37:03,713] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  33%|‚ñé| 869/2632 [10:08<20:33,  1.43it/s, v_num=t78b, train/loss=6.840][2023-07-09 13:37:17,424] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  34%|‚ñé| 899/2632 [10:28<20:10,  1.43it/s, v_num=t78b, train/loss=6.690][2023-07-09 13:37:37,304] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|‚ñé| 909/2632 [10:34<20:03,  1.43it/s, v_num=t78b, train/loss=6.720][2023-07-09 13:37:44,327] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|‚ñé| 919/2632 [10:41<19:56,  1.43it/s, v_num=t78b, train/loss=6.780][2023-07-09 13:37:51,018] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  35%|‚ñé| 929/2632 [10:48<19:49,  1.43it/s, v_num=t78b, train/loss=7.120][2023-07-09 13:37:58,071] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  36%|‚ñé| 949/2632 [11:02<19:35,  1.43it/s, v_num=t78b, train/loss=6.500][2023-07-09 13:38:11,832] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  36%|‚ñé| 959/2632 [11:09<19:27,  1.43it/s, v_num=t78b, train/loss=6.280][2023-07-09 13:38:18,842] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  39%|‚ñç| 1019/2632 [11:49<18:43,  1.44it/s, v_num=t78b, train/loss=6.310[2023-07-09 13:38:59,493] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  39%|‚ñç| 1039/2632 [12:03<18:29,  1.44it/s, v_num=t78b, train/loss=6.340[2023-07-09 13:39:13,084] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  41%|‚ñç| 1079/2632 [12:29<17:59,  1.44it/s, v_num=t78b, train/loss=6.440[2023-07-09 13:39:39,102] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  43%|‚ñç| 1119/2632 [12:56<17:29,  1.44it/s, v_num=t78b, train/loss=6.470[2023-07-09 13:40:05,687] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  45%|‚ñç| 1189/2632 [13:41<16:37,  1.45it/s, v_num=t78b, train/loss=6.500[2023-07-09 13:40:51,215] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  48%|‚ñç| 1269/2632 [14:34<15:39,  1.45it/s, v_num=t78b, train/loss=6.500[2023-07-09 13:41:43,571] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  51%|‚ñå| 1349/2632 [15:27<14:41,  1.46it/s, v_num=t78b, train/loss=6.780[2023-07-09 13:42:36,686] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  53%|‚ñå| 1399/2632 [15:59<14:05,  1.46it/s, v_num=t78b, train/loss=6.380[2023-07-09 13:43:09,266] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  54%|‚ñå| 1429/2632 [16:19<13:44,  1.46it/s, v_num=t78b, train/loss=6.910[2023-07-09 13:43:28,881] [WARNING] [stage3.py:1832:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  56%|‚ñå| 1469/2632 [16:45<13:16,  1.46it/s, v_num=t78b, train/loss=6.340[2023-07-09 13:43:55,315] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  56%|‚ñå| 1479/2632 [16:52<13:09,  1.46it/s, v_num=t78b, train/loss=6.030[2023-07-09 13:44:02,076] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  57%|‚ñå| 1509/2632 [17:12<12:48,  1.46it/s, v_num=t78b, train/loss=6.000[2023-07-09 13:44:22,208] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  60%|‚ñå| 1589/2632 [18:05<11:52,  1.46it/s, v_num=t78b, train/loss=6.000[2023-07-09 13:45:14,931] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  64%|‚ñã| 1679/2632 [19:04<10:49,  1.47it/s, v_num=t78b, train/loss=6.000[2023-07-09 13:46:13,682] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  66%|‚ñã| 1749/2632 [19:48<10:00,  1.47it/s, v_num=t78b, train/loss=6.120[2023-07-09 13:46:58,154] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  72%|‚ñã| 1889/2632 [21:17<08:22,  1.48it/s, v_num=t78b, train/loss=5.910[2023-07-09 13:48:26,841] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  81%|‚ñä| 2119/2632 [23:43<05:44,  1.49it/s, v_num=t78b, train/loss=5.780[2023-07-09 13:50:53,004] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  81%|‚ñä| 2129/2632 [23:50<05:37,  1.49it/s, v_num=t78b, train/loss=5.500[2023-07-09 13:50:59,615] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  82%|‚ñä| 2149/2632 [24:03<05:24,  1.49it/s, v_num=t78b, train/loss=5.840[2023-07-09 13:51:12,565] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0:  82%|‚ñä| 2159/2632 [24:09<05:17,  1.49it/s, v_num=t78b, train/loss=6.220[2023-07-09 13:51:19,446] [WARNING] [stage3.py:1832:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [29:12<00:00,  1.50it/s, v_num=t78b, train/loss=5.530\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:00<00:11,  2.35it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:01<00:15,  1.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:01<00:13,  1.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:02<00:12,  1.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:02<00:11,  1.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:03<00:10,  1.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:03<00:10,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:03<00:09,  2.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:04<00:08,  2.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:04<00:08,  2.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:05<00:07,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:05<00:06,  2.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:05<00:06,  2.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:06<00:05,  2.22it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:06<00:05,  2.23it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:07<00:04,  2.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:07<00:04,  2.25it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:08<00:04,  2.24it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:09<00:03,  2.00it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:09<00:03,  2.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:10<00:02,  2.08it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:10<00:02,  2.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:10<00:01,  2.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:11<00:01,  2.17it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:11<00:00,  2.20it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:11<00:00,  2.23it/s]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [29:31<00:00,  1.49it/s, v_num=t78b, train/loss=5.530\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [29:31<00:00,  1.49it/s, v_num=t78b, train/loss=5.530`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [29:31<00:00,  1.49it/s, v_num=t78b, train/loss=5.530\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.5625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 263\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.66522\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed (deepspeed_stage_3, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/ty2zt78b\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230709_132545-ty2zt78b/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 new_train.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepspeed 3 + offload\n",
    "Perform a full 1 epoch training run of training context size = 1024. With deepspeed 3 + offload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230709_143701-rpr0vv2m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-deepspeed (deepspeed_stage_3_offload, train-ctx=1024, data-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/rpr0vv2m\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 408.44it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-d9ebc51d67d5ce31_*_of_00128.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-161be9e5a7a54d76_*_of_00128.arrow\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-45459c41c653f279_*_of_00128.arrow\n",
      "[RWKV.model] Running RWKV model using 'torch-native' with torch '2.0.1+cu118'\n",
      "[rank: 0] Global seed set to 3941088705                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "[2023-07-09 14:37:20,779] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[rank: 1] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "[2023-07-09 14:37:36,613] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-09 14:37:36,616] [WARNING] [deepspeed.py:637:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[WARNING]: it is highly recommended to enable bptt_learning when used to deepspeed 2/3/offloading, otherwise an exception will occur when training with dataset records, larger then the configured context length (1024)\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.368435859680176 seconds\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4128847122192383 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.07565641403198242 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0709829330444336 seconds\n",
      "Parameter Offload: Total persistent parameters: 548864 in 268 params\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007009506225585938 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0007889270782470703 seconds\n",
      "\n",
      "  | Name   | Type       | Params | Params per Device\n",
      "----------------------------------------------------------\n",
      "0 | emb    | Embedding  | 102 M  | 51.5 M           \n",
      "1 | blocks | ModuleList | 1.3 B  | 654 M            \n",
      "2 | ln_out | LayerNorm  | 4.1 K  | 2.0 K            \n",
      "3 | head   | Linear     | 102 M  | 51.5 M           \n",
      "----------------------------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|                                         | 0/2632 [00:00<?, ?it/s][2023-07-09 14:40:07,222] [WARNING] [parameter_offload.py:86:_apply_to_tensors_only] A module has unknown inputs or outputs type (<class 'src.model.BlockState'>) and the tensors embedded in it cannot be detected. The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and output tensors and therefore may not get triggered properly.\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [1:42:38<00:00,  2.34s/it, v_num=vv2m, train/loss=5.5\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|‚ñã                  | 1/27 [00:18<08:13, 18.97s/it]\u001b[A\n",
      "Validation DataLoader 0:   7%|‚ñà‚ñç                 | 2/27 [00:19<04:05,  9.82s/it]\u001b[A\n",
      "Validation DataLoader 0:  11%|‚ñà‚ñà                 | 3/27 [00:20<02:40,  6.70s/it]\u001b[A\n",
      "Validation DataLoader 0:  15%|‚ñà‚ñà‚ñä                | 4/27 [00:20<01:58,  5.15s/it]\u001b[A\n",
      "Validation DataLoader 0:  19%|‚ñà‚ñà‚ñà‚ñå               | 5/27 [00:21<01:32,  4.22s/it]\u001b[A\n",
      "Validation DataLoader 0:  22%|‚ñà‚ñà‚ñà‚ñà‚ñè              | 6/27 [00:21<01:15,  3.60s/it]\u001b[A\n",
      "Validation DataLoader 0:  26%|‚ñà‚ñà‚ñà‚ñà‚ñâ              | 7/27 [00:22<01:03,  3.15s/it]\u001b[A\n",
      "Validation DataLoader 0:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 8/27 [00:22<00:53,  2.82s/it]\u001b[A\n",
      "Validation DataLoader 0:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé            | 9/27 [00:23<00:46,  2.58s/it]\u001b[A\n",
      "Validation DataLoader 0:  37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã           | 10/27 [00:23<00:40,  2.38s/it]\u001b[A\n",
      "Validation DataLoader 0:  41%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé          | 11/27 [00:24<00:35,  2.22s/it]\u001b[A\n",
      "Validation DataLoader 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 12/27 [00:25<00:31,  2.09s/it]\u001b[A\n",
      "Validation DataLoader 0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 13/27 [00:25<00:27,  1.98s/it]\u001b[A\n",
      "Validation DataLoader 0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé        | 14/27 [00:26<00:24,  1.88s/it]\u001b[A\n",
      "Validation DataLoader 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 15/27 [00:26<00:21,  1.80s/it]\u001b[A\n",
      "Validation DataLoader 0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã       | 16/27 [00:27<00:18,  1.72s/it]\u001b[A\n",
      "Validation DataLoader 0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 17/27 [00:28<00:16,  1.65s/it]\u001b[A\n",
      "Validation DataLoader 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      | 18/27 [00:28<00:14,  1.60s/it]\u001b[A\n",
      "Validation DataLoader 0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã     | 19/27 [00:29<00:12,  1.54s/it]\u001b[A\n",
      "Validation DataLoader 0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 20/27 [00:29<00:10,  1.49s/it]\u001b[A\n",
      "Validation DataLoader 0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 21/27 [00:30<00:08,  1.45s/it]\u001b[A\n",
      "Validation DataLoader 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 22/27 [00:30<00:07,  1.41s/it]\u001b[A\n",
      "Validation DataLoader 0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 23/27 [00:31<00:05,  1.37s/it]\u001b[A\n",
      "Validation DataLoader 0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/27 [00:32<00:04,  1.34s/it]\u001b[A\n",
      "Validation DataLoader 0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 25/27 [00:32<00:02,  1.31s/it]\u001b[A\n",
      "Validation DataLoader 0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 26/27 [00:33<00:01,  1.28s/it]\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [1:44:20<00:00,  2.38s/it, v_num=vv2m, train/loss=5.5\u001b[A\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [1:44:20<00:00,  2.38s/it, v_num=vv2m, train/loss=5.5`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|‚ñà| 2632/2632 [1:44:20<00:00,  2.38s/it, v_num=vv2m, train/loss=5.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: | 0.026 MB of 0.026 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 26\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 52\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.59375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 263\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.66782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33minfctx-deepspeed (deepspeed_stage_3_offload, train-ctx=1024, data-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/rpr0vv2m\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230709_143701-rpr0vv2m/logs\u001b[0m\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.11/threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 257, in check_network_status\n",
      "    self._loop_check_status(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 213, in _loop_check_status\n",
      "    local_handle = request()\n",
      "                   ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\", line 797, in deliver_network_status\n",
      "    return self._deliver_network_status(status)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 601, in _deliver_network_status\n",
      "    return self._deliver_record(record)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 560, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!cd ../../RWKV-v4neo && \\\n",
    "    export RWKV_JIT_ON=0 && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python3 new_train.py fit \\\n",
    "        -c ../notebook/trainer-validation/config/baseline-1024.yaml \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} (deepspeed_stage_3_offload, train-ctx=1024, data-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"deepspeed_stage_3_offload\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
