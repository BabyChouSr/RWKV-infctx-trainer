{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfCtx trainer baseline setup\n",
    "The trainer validation is mostly done using the either of the following\n",
    "\n",
    "**1B5 model wtih**\n",
    "- Layer count: 24\n",
    "- Embed size: 2048\n",
    "\n",
    "**L6-D512 model with**\n",
    "- Layer count: 6\n",
    "- Embed size: 512\n",
    "\n",
    "Typically with the following dataset\n",
    "- \"teven/enwiki_10k\" dataset, chunked to 1024 token sizes\n",
    "\n",
    "The following notebook, helps perform the basic download and setup for the \"init model\", and \"test dataset\". Which is used as a reference point for all other validation processes (unless stated otherwise)\n",
    "\n",
    "Generally you only need to do this once\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps\n",
    ">\n",
    "> All training runs (except dryrun) is configured to log to weights and bias, comment out the logger in the config file if you want to avoid this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Preparing the init model and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-17 10:17:58--  https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.108.87, 99.84.108.70, 99.84.108.55, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.108.87|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1692526678&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MjUyNjY3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzBlYzcyMTRlZDE2NzM3YTYzNDgyNTRlNmY5NmQ4Y2RjMDRkM2I1ZWZiZDVmNTNmZTkzMzc2MDdlYTQyYjViOWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=g9IfZ9eICXgcNS0m36nVyHhxIP5u3pUfaT2IDBlrXhrrkVscBUId3iFp9nqfofn%7E4BmPD7RPmJ5jBPqAVOWxaI5NX9Uvf6kyxGfCQm5dXWIZUfXatwNxZK8i13vHZBe2lkc0lXuphB9Bgui-9IBpHWezBj3MmDjY7MtrTBDYF8KIQYPhXW4rTvzj1%7EiPzVZROj2rrVPizS4W2JjqwuL8PXeoBrLPQyTLkPT%7EhEWpiTFedPi4AVps8JPFL9e3%7EoQRYw-8rMF0QpHSsVgy4IGvLNxTvc-ZmlaFEtDIk7l-4v0j6vK8zF6flf%7EchnB5HjYB1B33Y8n-DKqostfnUuZ%7EZA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-08-17 10:17:58--  https://cdn-lfs.huggingface.co/repos/cb/ef/cbef09abb2634a3375b28868bffa285226dfeabedec89b28c2fb302221164d66/0ec7214ed16737a6348254e6f96d8cdc04d3b5efbd5f53fe9337607ea42b5b9f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Echo-A-1B5-Init.pth%3B+filename%3D%22Echo-A-1B5-Init.pth%22%3B&Expires=1692526678&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MjUyNjY3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9jYi9lZi9jYmVmMDlhYmIyNjM0YTMzNzViMjg4NjhiZmZhMjg1MjI2ZGZlYWJlZGVjODliMjhjMmZiMzAyMjIxMTY0ZDY2LzBlYzcyMTRlZDE2NzM3YTYzNDgyNTRlNmY5NmQ4Y2RjMDRkM2I1ZWZiZDVmNTNmZTkzMzc2MDdlYTQyYjViOWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=g9IfZ9eICXgcNS0m36nVyHhxIP5u3pUfaT2IDBlrXhrrkVscBUId3iFp9nqfofn%7E4BmPD7RPmJ5jBPqAVOWxaI5NX9Uvf6kyxGfCQm5dXWIZUfXatwNxZK8i13vHZBe2lkc0lXuphB9Bgui-9IBpHWezBj3MmDjY7MtrTBDYF8KIQYPhXW4rTvzj1%7EiPzVZROj2rrVPizS4W2JjqwuL8PXeoBrLPQyTLkPT%7EhEWpiTFedPi4AVps8JPFL9e3%7EoQRYw-8rMF0QpHSsVgy4IGvLNxTvc-ZmlaFEtDIk7l-4v0j6vK8zF6flf%7EchnB5HjYB1B33Y8n-DKqostfnUuZ%7EZA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.49, 108.138.64.121, 108.138.64.36, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.49|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3030345861 (2.8G) [binary/octet-stream]\n",
      "Saving to: ‘Echo-A-1B5-Init.pth’\n",
      "\n",
      "Echo-A-1B5-Init.pth 100%[===================>]   2.82G  45.6MB/s    in 60s     \n",
      "\n",
      "2023-08-17 10:18:58 (48.6 MB/s) - ‘Echo-A-1B5-Init.pth’ saved [3030345861/3030345861]\n",
      "\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 2.9G Jun 22 04:41 ../../model/Echo-A-1B5-Init.pth\n"
     ]
    }
   ],
   "source": [
    "# First lets setup the various directories, and get the blank init model, these init model was generated\n",
    "# using the original RWKV-LM repo (as at this point of writing, this repo cannot init a model)\n",
    "# As such I have preinitialized these blank models and uploaded them to HF for convinence\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "!cd ../../model/ && wget -nc https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-A-1B5-Init.pth\n",
    "!ls -alh ../../model/Echo-A-1B5-Init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-30 14:30:20,457] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.1.0.dev20230706'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_2048_bf16...\n",
      "50277 512   -0.1 emb.weight\n",
      "512   512   0    blocks.0.att.key.weight\n",
      "512   512   1.0  blocks.0.att.value.weight\n",
      "512   512   0    blocks.0.att.receptance.weight\n",
      "512   512   0    blocks.0.att.output.weight\n",
      "2048  512   1.0  blocks.0.ffn.key.weight\n",
      "512   512   0    blocks.0.ffn.receptance.weight\n",
      "512   2048  0    blocks.0.ffn.value.weight\n",
      "512   512   0    blocks.1.att.key.weight\n",
      "512   512   1.0  blocks.1.att.value.weight\n",
      "512   512   0    blocks.1.att.receptance.weight\n",
      "512   512   0    blocks.1.att.output.weight\n",
      "2048  512   1.0  blocks.1.ffn.key.weight\n",
      "512   512   0    blocks.1.ffn.receptance.weight\n",
      "512   2048  0    blocks.1.ffn.value.weight\n",
      "512   512   0    blocks.2.att.key.weight\n",
      "512   512   1.0  blocks.2.att.value.weight\n",
      "512   512   0    blocks.2.att.receptance.weight\n",
      "512   512   0    blocks.2.att.output.weight\n",
      "2048  512   1.0  blocks.2.ffn.key.weight\n",
      "512   512   0    blocks.2.ffn.receptance.weight\n",
      "512   2048  0    blocks.2.ffn.value.weight\n",
      "512   512   0    blocks.3.att.key.weight\n",
      "512   512   1.0  blocks.3.att.value.weight\n",
      "512   512   0    blocks.3.att.receptance.weight\n",
      "512   512   0    blocks.3.att.output.weight\n",
      "2048  512   1.0  blocks.3.ffn.key.weight\n",
      "512   512   0    blocks.3.ffn.receptance.weight\n",
      "512   2048  0    blocks.3.ffn.value.weight\n",
      "512   512   0    blocks.4.att.key.weight\n",
      "512   512   1.0  blocks.4.att.value.weight\n",
      "512   512   0    blocks.4.att.receptance.weight\n",
      "512   512   0    blocks.4.att.output.weight\n",
      "2048  512   1.0  blocks.4.ffn.key.weight\n",
      "512   512   0    blocks.4.ffn.receptance.weight\n",
      "512   2048  0    blocks.4.ffn.value.weight\n",
      "512   512   0    blocks.5.att.key.weight\n",
      "512   512   1.0  blocks.5.att.value.weight\n",
      "512   512   0    blocks.5.att.receptance.weight\n",
      "512   512   0    blocks.5.att.output.weight\n",
      "2048  512   1.0  blocks.5.ffn.key.weight\n",
      "512   512   0    blocks.5.ffn.receptance.weight\n",
      "512   2048  0    blocks.5.ffn.value.weight\n",
      "50277 512   0.5  head.weight\n"
     ]
    }
   ],
   "source": [
    "# Lets initialized the L6-D512 model with the init_model.py code\n",
    "!cd ../../RWKV-v4neo/ && python3 init_model.py --n_layer 6 --n_embd 512 --vocab_size neox ../model/L6-D512-neox-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 967.32it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/trainer-validation/config/baseline-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer Code validation via dryrun\n",
    "\n",
    "The following dryrun, help do a basic check that the existing trainer code changes are valid across 2 * 2 data samples.\n",
    "\n",
    "If this check fail, its most probably a code / envrionment setup issue (no further checks needed)\n",
    "\n",
    "It does not log the run the W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes\n",
      "# https://github.com/RWKV/RWKV-infctx-trainer\n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 10:20:30,597] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/trainer-validation/config/baseline-dryrun.yaml'], args=['fit', '-c', '../notebook/trainer-validation/config/baseline-dryrun.yaml'].\n",
      "  rank_zero_warn(\n",
      "Global seed set to 3941088705\n",
      "[RWKV.model]: Preloading model from '../model/Echo-A-1B5-Init.pth'\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_128_bf16...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_128_bf16/build.ninja...\n",
      "Building extension module wkv_128_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_128_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=128 -c /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    1536 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_128_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=128 -c /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_128_bf16.so\n",
      "Loading extension module wkv_128_bf16...\n",
      "[RWKV.model]: Loading model weights ( L24-D2048-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/connector.py:554: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Downloading readme: 100%|██████████████████████| 424/424 [00:00<00:00, 5.22MB/s]\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/15.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 15.2M/15.2M [00:00<00:00, 86.5MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:00<00:00,  5.64it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2036.07it/s]\n",
      "Setting num_proc from 32 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 100%|██| 10000/10000 [00:00<00:00, 105017.23 examples/s]\n",
      "Map (num_proc=32): 100%|██████████| 10000/10000 [00:15<00:00, 663.91 examples/s]\n",
      "Filter (num_proc=32): 100%|██████| 10000/10000 [00:06<00:00, 1466.05 examples/s]\n",
      "Map (num_proc=32): 100%|███████████| 8110/8110 [00:08<00:00, 1004.07 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|█| 5308/5308 [00:00<00:00, 61374.02 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█| 54/54 [00:00<00:00, 20871.03 examples/s\n",
      "[rank: 0] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-17 10:21:43,058] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Creating extension directory /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
      "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
      "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 29.484052896499634 seconds\n",
      "[RWKV.model][rank=0] Loaded optimizer (linear schedule) ...\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%|   | 4/5308 [00:57<21:17:58, 14.46s/it, v_num=3, train/loss=9.620]`Trainer.fit` stopped: `max_steps=2` reached.\n",
      "Epoch 0:   0%|   | 4/5308 [00:57<21:18:04, 14.46s/it, v_num=3, train/loss=9.620]\n"
     ]
    }
   ],
   "source": [
    "# Validate source code and env is working, by doing a short 2 sample dryrun\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/trainer-validation/config/baseline-dryrun.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Baseline full context (1024) training\n",
    "\n",
    "Perform a full 1 epoch training run of training context size = 1024. Ensuring all data samples fit within the allocated training size. And is used as the baseline loss comparision for several experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-01 20:53:18,310] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Global seed set to 3941088705\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230701_205320-k8flu72z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/k8flu72z\u001b[0m\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 675.41it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-3d43d1724bef83d7_*_of_00016.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-5033407f38c97f24.arrow\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-78e7f3a5f1679aa4_*_of_00016.arrow\n",
      "/home/picocreator/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[rank: 0] Global seed set to 3941088705\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-01 20:53:34,204] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "/home/picocreator/anaconda3/envs/rwkv-exp/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/picocreator/rwkv-proj/infctx-dev/checkpoint/trainer-validaiton/infctx-validation-full exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.312431812286377 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0: 100%|█| 5318/5318 [51:02<00:00,  1.74it/s, v_num=u72z, train/loss=5.940\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:00<00:08,  5.98it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:07,  6.81it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:00<00:07,  7.14it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:00<00:06,  7.32it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:00<00:06,  7.41it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:00<00:06,  7.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:00<00:06,  7.54it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:01<00:06,  7.59it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:01<00:05,  7.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:01<00:05,  7.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:01<00:05,  7.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:01<00:05,  7.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:01<00:05,  7.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:01<00:05,  7.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:01<00:05,  7.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:02<00:04,  7.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:02<00:04,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:02<00:04,  7.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:02<00:04,  7.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:02<00:04,  7.74it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:02<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:02<00:04,  7.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:02<00:03,  7.76it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:03<00:03,  7.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:03<00:03,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:03<00:03,  7.78it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:03<00:03,  7.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:03<00:03,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:03<00:03,  7.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:03<00:03,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:03<00:02,  7.81it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:04<00:02,  7.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:04<00:02,  7.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:04<00:02,  7.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:04<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:05<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:05<00:01,  7.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [00:05<00:01,  7.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [00:05<00:01,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [00:05<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [00:06<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [00:06<00:00,  7.87it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [00:06<00:00,  7.88it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940\u001b[A\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 5318/5318 [51:11<00:00,  1.73it/s, v_num=u72z, train/loss=5.940\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ██▇▇█▆▇▆▄▅▄▃▄▄▄▄▂▃▃▃▃▃▃▄▁▂▂▃▃▂▂▂▂▃▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 1023\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 53\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 5.96875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 443\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0004\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 5.67332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33minfctx-validation-full (train-ctx=1024, data-ctx=1024, bs=12)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-InfCtx-Validation/runs/k8flu72z\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230701_205320-k8flu72z/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Full training run\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/trainer-validation/config/baseline-1024.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
