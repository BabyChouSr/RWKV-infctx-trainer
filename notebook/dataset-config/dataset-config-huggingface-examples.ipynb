{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HF (Hugging Face) based datasets with infctx trainer\n",
    "\n",
    "The infctx trainer makes a huge shift towards a HF focus dataset parser, with several pros and cons. This note book aims to cover all the common use cases for dataset handling and processing, that is supported by this trainer code.\n",
    "\n",
    "Because there are multiple possible strategy for parsing of the dataset, they are evaluated in the following order by default\n",
    "\n",
    "- multi_column_keys (used if any collumn matches)\n",
    "- prompt & completion (used if both collumn exists)\n",
    "- text (default baseline)\n",
    "\n",
    "We would be going through how the above dataset processing strategies work, starting with the default baseline.\n",
    "\n",
    "> Important note: These example focuses only on how to configure your dataset, and does not properly perform checkmarking - for trainer configurations refer to the training notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intial setup\n",
    "\n",
    "Before we go into the dataset setup, lets perform an initial setup for all the folders we need, and a small toy model which we would use throughout the various examples within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 11:23:28,641] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-neox-init.pth\n",
      "Vocab size: 50277\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n",
      "[2023-08-04 11:23:31,917] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-world-init.pth\n",
      "Vocab size: 65529\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n",
      "[2023-08-04 11:23:35,233] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 6\n",
      "Embedding size: 512\n",
      "Output model path: ../model/L6-D512-V20259-init.pth\n",
      "Vocab size: 20259\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Setup the folders we will need\n",
    "!mkdir -p ../../model/\n",
    "!mkdir -p ../../datapath/\n",
    "!mkdir -p ../../checkpoint/\n",
    "\n",
    "# Initialized a simple L6-D512 model, for both the v4 neox (50277) tokenizer\n",
    "!cd ../../RWKV-v4neo/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size neox --skip-if-exists ../model/L6-D512-neox-init.pth\n",
    "\n",
    "# and rwkv world (65529) tokenizers\n",
    "!cd ../../RWKV-v4neo/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size world --skip-if-exists ../model/L6-D512-world-init.pth\n",
    "\n",
    "# If you have a custom vocab size, you can indicate accordingly as well with an int\n",
    "!cd ../../RWKV-v4neo/ && python3 ./init_model.py --n_layer 6 --n_embd 512 --vocab_size 20259 --skip-if-exists ../model/L6-D512-V20259-init.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using a text dataset\n",
    "\n",
    "The following is the `example-enwiki.yaml` settings, for using a textual dataset via huggingface, with most of the comments removed\n",
    "\n",
    "---\n",
    "```yaml\n",
    "trainer:\n",
    "  max_steps: 10\n",
    "  target_batch_size: 32\n",
    "model:\n",
    "  load_model: ../model/L6-D512-neox-init.pth\n",
    "  ctx_len: 1024\n",
    "  lr_init: 3e-4\n",
    "  bptt_learning: true\n",
    "  bptt_learning_range: -1\n",
    "data:\n",
    "  data_path: ../datapath/enwiki_10k_neox_1024/\n",
    "\n",
    "  source: \"teven/enwiki_10k\"\n",
    "  # # Additional source dataset params, used to grab subsets\n",
    "  # source_dataset_params:\n",
    "  #   language: en\n",
    "\n",
    "  tokenizer: neox\n",
    "  min_token_size: 64\n",
    "  max_token_size: -1\n",
    "  text_rechunk_size: 2048\n",
    "  text_rechunk_force: true\n",
    "  custom_text_key: 'text'\n",
    "  test_split: 0.01\n",
    "  test_split_shuffle: false\n",
    "```\n",
    "---\n",
    "\n",
    "### Understanding the `data` config, for textual datasets\n",
    "\n",
    "Lets go through each of data parameter settings and what they mean...\n",
    "\n",
    "**data.data_path** \n",
    "\n",
    "This is where the HF datapath is saved, when used against existing HF data sources. This is a requried parameter\n",
    "\n",
    "**data.source** \n",
    "\n",
    "This can be configured either as a hugging face dataset (eg. `teven/enwiki_10k`) or if intended to be used with local files `text / json / csv / pandas` or their respective file paths (you can point to a single `.txt/.jsonl` file and it should 'work', see the local file examples for more details)\n",
    "\n",
    "**data.source_dataset_params** \n",
    "\n",
    "Additional params to configure the huggingface `load_dataset` command. This is only useful for larger dataset which supports such parameters, to filter out a subsets specifically (defaults to empty object)\n",
    "\n",
    "**data.tokenizer**\n",
    "\n",
    "The tokenizer to use for the dataset, use either `neox` or `world` for the respective RWKV models. For custom HF tokenizer refer to the custom tokenizer example. (defaults to neox)\n",
    "\n",
    "**data.min_token_size/max_token_size**\n",
    "\n",
    "Scans the given dataset, and skips datasamples that fail to meet the given criteria. This is mostly useful for filtering small low quality datasamples in large datasets, or large datasample beyond what you intend to support. (this is done before rechunking if enabled, defaults to -1 which support all)\n",
    "\n",
    "**data.text_rechunk_force**\n",
    "\n",
    "Enable text rechunking, this means, all the filtered datasamples will be merged together, with a new line between them. Before being split again by the rechunk size. This is mostly useful for large corpus of raw text data, and is consistent with how existing foundation model are trained from raw text files. This also allows more efficient training process (tokens/second), as each datasample will have the exact same token count. (Disabled by default unless your source is literally a \".txt\" file)\n",
    "\n",
    "**data.text_rechunk_size** \n",
    "\n",
    "Number of tokens each datasample should have after rechunking. Recommended sizes is the context size you intend the model to support (ie. 2048, 4096, etc)\n",
    "\n",
    "**data.custom_text_key** \n",
    "\n",
    "For huggingface datasets (or json/csv) by default we would use the `text` collumn if its avaliable. However some dataset store their text in a different collumn (eg. `code`). This allow you to choose which collumn would you like to use the text from. Note for more complicated instruct/input/output examples, you will want to see the 'multi_column' guide/examples instead.\n",
    "\n",
    "**data.test_split**\n",
    "\n",
    "Important Note: this is ignored, if the HF dataset has an inbuilt test split.\n",
    "\n",
    "If configured as a floating number between 0.0 and 1.0, it will be the percentage (0.1 is 10%) of the test data that is used for test validation.\n",
    "\n",
    "If configured as an int number, it will be the number of samples.\n",
    "\n",
    "Due to some limitations in the current trainer code, even if its set as 0, we will use a single data sample for the test split.\n",
    "\n",
    "This defaults to 0.01 or 1%\n",
    "\n",
    "**data.test_shuffle**\n",
    "\n",
    "Perform a dataset shuffle before test split, this defaults to False.\n",
    "\n",
    "Note, this is not a truely random shuffle, but a detriministic shuffle. To ensure a consistent result.\n",
    "\n",
    "### Optimizing the `model.bptt_*` mode according to your dataset config ....\n",
    "\n",
    "**model.load_model** \n",
    "\n",
    "This is the `model.pth` file you start the training process from. This can be an exisitng model you are finetuning from, or a new model that you initalized with `init_model.py` script.\n",
    "\n",
    "**model.ctx_len** \n",
    "\n",
    "This is the training context length used in the training process. For the infctx trainer, your data samples can be larger then the training context length. If so, the data sample is split into chunks accordingly, and trained in parts (with bptt_learning enabled, which it is by default)\n",
    "\n",
    "This is ultimately a tradeoff between VRAM usage vs GPU compute usage, while you can save VRAM usage, this comes an increased compute cost, as first few chunks will need to recalculated multiple times for each subsequent chunk. There are also been recorded minor loss learning penalty especially for small context sizes.\n",
    "\n",
    "As such it is always recommended to configure this to be as large as what can be supported by your GPU in the power of 2 (1024,2048,4096,...) with some healthy vram buffer for checkpoints and gradients, and up to your dataset sample size (as its pointless to go beyond that)\n",
    "\n",
    "Typically this is 2048, 4096, or 8192 for ML training GPUS (24GB vram and above). For consumer GPUS, anything less then 512 is not recommended, due to compounded loss learning penalty involved when used with large data samples.\n",
    "\n",
    "**model.bptt_learning**\n",
    "\n",
    "Enabled by default, this is the core feature of infctx trainer. If your training ctx length is equal to your dataset context length, you can disable bptt_learning for an insignificant speed boost (barely measurable).\n",
    "\n",
    "In most cases its better to just set bptt_learning_range to 1 instead of switching it off\n",
    "\n",
    "**model.bptt_learning_range**\n",
    "\n",
    "`bptt_learning_range: -1` will work by default for all use cases. On a single GPU.\n",
    "\n",
    "However, when training across multiple GPUs `bptt_learning_range: -1` has a small performance penalty in which it needs to syncronize the number of chunks across multiple GPUs. \n",
    "\n",
    "This is an issue especially, when training with mixed dataset size, if a single GPU is stuck with a significantly larger document with many chunks, all the other GPUs maybe stuck waiting for that one GPU to complete.\n",
    "\n",
    "In most cases this would be an acceptable compromise with mixed sized dataset. However if your dataset is of fixed size. Especially with 'rechunking' enabled. You can optimize multiple GPU training by configuring the learning range to be exactly equals to the number of chunk (eg: learning_range = 4, for data size of 4096, training ctx len of 1024)\n",
    "\n",
    "You can also configure the range to be less then the data sample size, in which the learning process will only happen for the last X configured chunks. This is not as bad as it sounds, and has it uses cases (which will be documented seperately)\n",
    "\n",
    "### Download and preload the datapath from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 11:31:18,816] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1022.75it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/dataset-config/example-enwiki.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 13:55:44,637] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/dataset-config/mini-enwiki.yaml'], args=['fit', '-c', '../notebook/dataset-config/mini-enwiki.yaml'].\n",
      "  rank_zero_warn(\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1792348788\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1792348788\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_1024_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 965.32it/s]\n",
      "[rank: 0] Global seed set to 1792348788                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-04 13:56:11,934] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  3.000e-04 (0.0003)\n",
      "    - lr_final: 3.000e-04 (0.0003)\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.312164068222046 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0:  12%|▍   | 320/2657 [00:29<03:37, 10.74it/s, v_num=1, train/loss=9.380]/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=10` reached.\n",
      "Epoch 0:  12%|▍   | 320/2657 [00:30<03:41, 10.53it/s, v_num=1, train/loss=9.380]\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-enwiki.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using prompt completion pair dataset\n",
    "\n",
    "However, beyond foundation model training, for finetuning. One common format is the `prompt` and `completion` pair. This is supported out of the box.\n",
    "\n",
    "An example of the prompt/completion pair as followed\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"prompt\": \"What is the dominant emotion of the user? I am happy. Output:\",\n",
    "  \"completion\": \" Happy<|endoftext|>\"\n",
    "}\n",
    "```\n",
    "\n",
    "Setting this up, as simple as the following\n",
    "\n",
    "---\n",
    "```yaml\n",
    "trainer:\n",
    "  max_steps: 10\n",
    "  target_batch_size: 32\n",
    "model:\n",
    "  load_model: ../model/L6-D512-neox-init.pth\n",
    "  ctx_len: 1024\n",
    "  lr_init: 1e-4\n",
    "  bptt_learning: true\n",
    "  bptt_learning_range: -1\n",
    "data:\n",
    "  data_path: ../datapath/self-instruct-base/\n",
    "  source: \"eastwind/self-instruct-base\"\n",
    "  tokenizer: neox\n",
    "  disable_prompt_completion_mask: false\n",
    "```\n",
    "---\n",
    "\n",
    "**data.disable_prompt_completion_mask**\n",
    "\n",
    "If the dataset uses prompt/completion data layout. By default it would be used in place of the text collumn. Typically, no additional configuration required.\n",
    "\n",
    "However, the default prompt completion behaviour, is that the text on the prompt half is \"learning masked\" disabled, while the text on the completion half has the \"learning mask\" enabled.\n",
    "\n",
    "In practise, the model will not learn how to generate the prompt as an output. And the learnings are focused on the completion half.\n",
    "\n",
    "### Preload the dataset and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 11:31:18,816] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/teven___parquet/teven--enwiki_10k-de63a925546e70ab/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1022.75it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/dataset-config/example-hf-prompt-completion.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 15:34:37,777] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/dataset-config/example-hf-prompt-completion.yaml'], args=['fit', '-c', '../notebook/dataset-config/example-hf-prompt-completion.yaml'].\n",
      "  rank_zero_warn(\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2828405798\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2828405798\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=4096 -c /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 3 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    49152 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=4096 -c /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_4096_bf16.so\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Downloading readme: 100%|██████████████████████| 950/950 [00:00<00:00, 9.91MB/s]\n",
      "Downloading and preparing dataset json/eastwind--self-instruct-base to /home/picocreator/.cache/huggingface/datasets/eastwind___json/eastwind--self-instruct-base-bb475a839cb63555/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/24.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                  | 1.02k/24.1M [00:00<1:31:35, 4.39kB/s]\u001b[A\n",
      "Downloading data:   0%|                    | 34.8k/24.1M [00:00<04:37, 86.8kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 104k/24.1M [00:00<02:11, 183kB/s]\u001b[A\n",
      "Downloading data:   1%|▏                     | 209k/24.1M [00:00<01:23, 287kB/s]\u001b[A\n",
      "Downloading data:   2%|▍                     | 453k/24.1M [00:01<00:42, 558kB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 923k/24.1M [00:01<00:18, 1.28MB/s]\u001b[A\n",
      "Downloading data:   5%|▉                   | 1.10M/24.1M [00:01<00:17, 1.31MB/s]\u001b[A\n",
      "Downloading data:   8%|█▌                  | 1.88M/24.1M [00:01<00:08, 2.65MB/s]\u001b[A\n",
      "Downloading data:   9%|█▊                  | 2.23M/24.1M [00:01<00:08, 2.67MB/s]\u001b[A\n",
      "Downloading data:  16%|███▏                | 3.77M/24.1M [00:01<00:03, 5.62MB/s]\u001b[A\n",
      "Downloading data:  18%|███▋                | 4.44M/24.1M [00:01<00:03, 5.42MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▎             | 7.59M/24.1M [00:02<00:01, 9.09MB/s]\u001b[A\n",
      "Downloading data:  47%|█████████▎          | 11.2M/24.1M [00:02<00:01, 11.6MB/s]\u001b[A\n",
      "Downloading data:  63%|████████████▋       | 15.3M/24.1M [00:02<00:00, 13.6MB/s]\u001b[A\n",
      "Downloading data:  80%|████████████████    | 19.4M/24.1M [00:02<00:00, 14.8MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 24.1M/24.1M [00:03<00:00, 7.31MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:06<00:00,  6.37s/it]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2516.08it/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset json downloaded and prepared to /home/picocreator/.cache/huggingface/datasets/eastwind___json/eastwind--self-instruct-base-bb475a839cb63555/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 135.38it/s]\n",
      "[rank: 0] Global seed set to 2828405798                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-04 15:35:27,989] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  1.000e-04 (0.0001)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3117449283599854 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0:   0%| | 320/81785 [00:20<1:28:56, 15.26it/s, v_num=2, train/loss=6.410]/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=10` reached.\n",
      "Epoch 0:   0%| | 320/81785 [00:21<1:31:14, 14.88it/s, v_num=2, train/loss=6.410]\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-hf-prompt-completion.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using multi column keys\n",
    "\n",
    "What if you want alternative format, with more complicated layouts. Like instruction/input/output (or other formats)\n",
    "\n",
    "You can use the `multi_column_keys` which gives you precise control over how each data sample is processed.\n",
    "\n",
    "For example the following is an example data record for dolly instruction set (simplified)\n",
    "\n",
    "```\n",
    "{\n",
    "    \"category\": \"closed_qa\"\t,\n",
    "    \"instruction\": \"When did Virgin Australia start operating?\",\n",
    "    \"input\": \"Virgin Australia, the trading ....\",\n",
    "    \"output\": \"Virgin Australia commenced services on ...\"\n",
    "}\n",
    "```\n",
    "\n",
    "If using the default settings as shown below, this will get converted into the following training text (ignoring masking)\n",
    "\n",
    "```\n",
    "Instruction:\n",
    "When did Virgin Australia start operating?\n",
    "\n",
    "Input:\n",
    "Virgin Australia, the trading ....\n",
    "\n",
    "Output:\n",
    "Virgin Australia commenced services on ...\n",
    "```\n",
    "\n",
    "We can support the following dataset with the following settings\n",
    "\n",
    "---\n",
    "```yaml\n",
    "trainer:\n",
    "  max_steps: 10\n",
    "  target_batch_size: 32\n",
    "model:\n",
    "  load_model: ../model/L6-D512-neox-init.pth\n",
    "  ctx_len: 1024\n",
    "  lr_init: 1e-4\n",
    "  bptt_learning: true\n",
    "  bptt_learning_range: -1\n",
    "data:\n",
    "  data_path: ../datapath/self-instruct-base/\n",
    "  source: \"c-s-ale/dolly-15k-instruction-alpaca-format\"\n",
    "  tokenizer: neox\n",
    "  multi_column_keys: ['instruction', 'input', 'output']\n",
    "  multi_column_prefix: ['Instruction:\\n', 'Input:\\n', 'Output:\\n']\n",
    "  multi_column_train_mask: [true, false, true]\n",
    "  multi_column_separator: '\\n\\n'\n",
    "```\n",
    "---\n",
    "\n",
    "**data.multi_column_keys**\n",
    "\n",
    "Defaults to: `['instruction', 'input', 'output']`\n",
    "\n",
    "List of keys to detect, and use for your text data training. Requires atleast one column to exist, all other collumns will be ignored. Columns are matched in the given order.\n",
    "\n",
    "**data.multi_column_prefix**\n",
    "\n",
    "Defaults to `['Instruction:\\n', 'Input:\\n', 'Output:\\n']`\n",
    "\n",
    "For each matching column found, append the following string as a prefix in the matching array position to `multi_column_keys`\n",
    "\n",
    "**data.multi_column_train_mask**\n",
    "\n",
    "Defaults to `[true, false, true]`\n",
    "\n",
    "For each matching column found, either apply the training mask where the model will learn from (true), or to ignore in the learning process (false).\n",
    "\n",
    "**data.multi_column_separator**\n",
    "\n",
    "Defaults to: `\\n\\n`\n",
    "\n",
    "String to append inbetween each matching multi column\n",
    "\n",
    "### Preload the dataset and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 18:06:19,758] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Found cached dataset parquet (/home/picocreator/.cache/huggingface/datasets/c-s-ale___parquet/c-s-ale--dolly-15k-instruction-alpaca-format-9dfbb23260d63d9d/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 41.55it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd ../../RWKV-v4neo && python3 preload_datapath.py ../notebook/dataset-config/example-hf-multi-column-keys.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-04 15:34:37,777] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '../notebook/dataset-config/example-hf-prompt-completion.yaml'], args=['fit', '-c', '../notebook/dataset-config/example-hf-prompt-completion.yaml'].\n",
      "  rank_zero_warn(\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 2828405798\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 2828405798\n",
      "[RWKV.model]: Preloading model from '../model/L6-D512-neox-init.pth'\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_4096_bf16/build.ninja...\n",
      "Building extension module wkv_4096_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=4096 -c /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 3 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    49152 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_4096_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/picocreator/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=4096 -c /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_4096_bf16.so\n",
      "Loading extension module wkv_4096_bf16...\n",
      "[RWKV.model]: Loading model weights ( L6-D512-V50277 )\n",
      "[RWKV.model]: Finished initial model load\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       32\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    32\n",
      "\n",
      "Downloading readme: 100%|██████████████████████| 950/950 [00:00<00:00, 9.91MB/s]\n",
      "Downloading and preparing dataset json/eastwind--self-instruct-base to /home/picocreator/.cache/huggingface/datasets/eastwind___json/eastwind--self-instruct-base-bb475a839cb63555/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                             | 0.00/24.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0%|                  | 1.02k/24.1M [00:00<1:31:35, 4.39kB/s]\u001b[A\n",
      "Downloading data:   0%|                    | 34.8k/24.1M [00:00<04:37, 86.8kB/s]\u001b[A\n",
      "Downloading data:   0%|                      | 104k/24.1M [00:00<02:11, 183kB/s]\u001b[A\n",
      "Downloading data:   1%|▏                     | 209k/24.1M [00:00<01:23, 287kB/s]\u001b[A\n",
      "Downloading data:   2%|▍                     | 453k/24.1M [00:01<00:42, 558kB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 923k/24.1M [00:01<00:18, 1.28MB/s]\u001b[A\n",
      "Downloading data:   5%|▉                   | 1.10M/24.1M [00:01<00:17, 1.31MB/s]\u001b[A\n",
      "Downloading data:   8%|█▌                  | 1.88M/24.1M [00:01<00:08, 2.65MB/s]\u001b[A\n",
      "Downloading data:   9%|█▊                  | 2.23M/24.1M [00:01<00:08, 2.67MB/s]\u001b[A\n",
      "Downloading data:  16%|███▏                | 3.77M/24.1M [00:01<00:03, 5.62MB/s]\u001b[A\n",
      "Downloading data:  18%|███▋                | 4.44M/24.1M [00:01<00:03, 5.42MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▎             | 7.59M/24.1M [00:02<00:01, 9.09MB/s]\u001b[A\n",
      "Downloading data:  47%|█████████▎          | 11.2M/24.1M [00:02<00:01, 11.6MB/s]\u001b[A\n",
      "Downloading data:  63%|████████████▋       | 15.3M/24.1M [00:02<00:00, 13.6MB/s]\u001b[A\n",
      "Downloading data:  80%|████████████████    | 19.4M/24.1M [00:02<00:00, 14.8MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 24.1M/24.1M [00:03<00:00, 7.31MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:06<00:00,  6.37s/it]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2516.08it/s]\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Dataset json downloaded and prepared to /home/picocreator/.cache/huggingface/datasets/eastwind___json/eastwind--self-instruct-base-bb475a839cb63555/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 135.38it/s]\n",
      "[rank: 0] Global seed set to 2828405798                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-04 15:35:27,989] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[RWKV.model][rank=0] Configuring optimizer ...\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  1.000e-04 (0.0001)\n",
      "    - lr_final: 1.000e-04 (0.0001)\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m cpu_adam cuda is missing or is incompatible with installed torch, only cpu ops can be compiled!\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3117449283599854 seconds\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(71960576, False), (3072, False), (3072, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 25.7 M\n",
      "1 | blocks | ModuleList | 20.5 M\n",
      "2 | ln_out | LayerNorm  | 1.0 K \n",
      "3 | head   | Linear     | 25.7 M\n",
      "--------------------------------------\n",
      "72.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "72.0 M    Total params\n",
      "287.867   Total estimated model params size (MB)\n",
      "Epoch 0:   0%| | 320/81785 [00:20<1:28:56, 15.26it/s, v_num=2, train/loss=6.410]/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_steps=10` reached.\n",
      "Epoch 0:   0%| | 320/81785 [00:21<1:31:14, 14.88it/s, v_num=2, train/loss=6.410]\n"
     ]
    }
   ],
   "source": [
    "# Validate the dataset is working, by doing a quick training run\n",
    "!cd ../../RWKV-v4neo && python3 lightning_trainer.py fit -c ../notebook/dataset-config/example-hf-multi-column-keys.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
