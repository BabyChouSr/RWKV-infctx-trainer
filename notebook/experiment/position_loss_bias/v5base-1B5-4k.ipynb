{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5-base 1B5 / embedding init-range 1e-01 / 4k\n",
    "This model is based on the RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the modified memory training for v5 models, across various initial embedding model weights\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_2_offload\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/position_loss_bias\n",
      "INFERENCE_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5\n",
      "TRAINER_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5\n",
      "PROJECT_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "EMBED_SCALE=0.1\n",
    "EMBED_SCALE_LABEL=str(EMBED_SCALE).replace(\".\", \"_\")\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-18 08:40:24,121] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "---- Initializing model ----\n",
      "No of layers: 24\n",
      "Embedding size: 2048\n",
      "Output model path: ../model/L24-D2048-E0_1-neox-v5base-init.pth\n",
      "Vocab size: 50277\n",
      "Emb scale: 0.1\n",
      "---- ----- ----\n",
      "Model exists, skipping init_model\n"
     ]
    }
   ],
   "source": [
    "# Init the model\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 ./init_model.py \\\n",
    "        --n_layer 24 --n_embd 2048 \\\n",
    "        --emb-scale \"{EMBED_SCALE}\" \\\n",
    "        --vocab_size neox --skip-if-exists \\\n",
    "        \"../model/L24-D2048-E{EMBED_SCALE_LABEL}-neox-v5base-init.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|█| 5308/5308 [00:00<00:00, 53354.69 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█| 54/54 [00:00<00:00, 9211.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset \n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/v5base-enwiki-4k.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enwiki 10k / p0.1 - ctx 4k training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_LOSS_BIAS: 0.1\n",
      "WANDB_PREFIX: v5base-1B5-E0.1-P0.1\n",
      "FILENAME_PREFIX: v5base-1B5-E0_1-P0_1\n"
     ]
    }
   ],
   "source": [
    "POS_LOSS_BIAS=0.1\n",
    "POS_LOSS_BIAS_LABEL=str(POS_LOSS_BIAS).replace(\".\", \"_\")\n",
    "\n",
    "WANDB_PREFIX=f\"v5base-1B5-E{EMBED_SCALE}-P{POS_LOSS_BIAS}\"\n",
    "FILENAME_PREFIX=f\"v5base-1B5-E{EMBED_SCALE_LABEL}-P{POS_LOSS_BIAS_LABEL}\"\n",
    "\n",
    "print(\"POS_LOSS_BIAS:\", POS_LOSS_BIAS)\n",
    "print(\"WANDB_PREFIX:\", WANDB_PREFIX)\n",
    "print(\"FILENAME_PREFIX:\", FILENAME_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-18 08:40:33,223] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/position_loss_bias/v5base-enwiki-1k.yaml', '--trainer.logger.init_args.name=v5base-1B5-E0.1-P0.1 - Enwiki-small-4k Foundation (train-ctx=4k, deepspeed_stage_2_offload)', '--trainer.strategy=deepspeed_stage_2_offload', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5base-1B5-E0_1-P0_1-enwiki-4k/', '--model.load_model=../model/L24-D2048-E0_1-neox-v5base-init.pth', '--model.position_loss_bias=0.1', '--model.ctx_len=4096', '--model.bptt_learning_range=1'], args=['fit', '-c', '/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/position_loss_bias/v5base-enwiki-1k.yaml', '--trainer.logger.init_args.name=v5base-1B5-E0.1-P0.1 - Enwiki-small-4k Foundation (train-ctx=4k, deepspeed_stage_2_offload)', '--trainer.strategy=deepspeed_stage_2_offload', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5base-1B5-E0_1-P0_1-enwiki-4k/', '--model.load_model=../model/L24-D2048-E0_1-neox-v5base-init.pth', '--model.position_loss_bias=0.1', '--model.ctx_len=4096', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3217984318\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3217984318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "cat: /sys/module/amdgpu/initstate: No such file or directory\n",
      "ERROR:root:Driver not initialized (amdgpu not found in modules)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230818_084035-pikwhw72\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5base-1B5-E0.1-P0.1 - Enwiki-small-4k Foundation (train-ctx=4k, deepspeed_stage_2_offload)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/pikwhw72\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█| 5308/5308 [00:00<00:00, 56828.80 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█| 54/54 [00:00<00:00, 8821.52 examples/s]\n",
      "[rank: 0] Global seed set to 3217984318\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-18 08:40:50,357] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3371102809906006 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1] and sizes[(1515106304, False), (1536, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   8%| | 400/5308 [06:19<1:17:39,  1.05it/s, v_num=hw72, train/loss=7.85/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0:  10%| | 530/5308 [10:12<1:32:04,  1.16s/it, v_num=hw72, train/loss=7.88\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|▎                  | 1/54 [00:00<00:16,  3.17it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|▋                  | 2/54 [00:00<00:15,  3.44it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 3/54 [00:00<00:14,  3.54it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|█▍                 | 4/54 [00:01<00:13,  3.59it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|█▊                 | 5/54 [00:01<00:13,  3.57it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 6/54 [00:01<00:13,  3.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  13%|██▍                | 7/54 [00:01<00:12,  3.62it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|██▊                | 8/54 [00:02<00:12,  3.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 9/54 [00:02<00:12,  3.65it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|███▎              | 10/54 [00:02<00:12,  3.66it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|███▋              | 11/54 [00:02<00:11,  3.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████              | 12/54 [00:03<00:11,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|████▎             | 13/54 [00:03<00:11,  3.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|████▋             | 14/54 [00:03<00:10,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████             | 15/54 [00:04<00:10,  3.69it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|█████▎            | 16/54 [00:04<00:10,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|█████▋            | 17/54 [00:04<00:10,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████            | 18/54 [00:04<00:09,  3.70it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|██████▎           | 19/54 [00:05<00:09,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  37%|██████▋           | 20/54 [00:05<00:09,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████           | 21/54 [00:05<00:08,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|███████▎          | 22/54 [00:05<00:08,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|███████▋          | 23/54 [00:06<00:08,  3.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████          | 24/54 [00:06<00:08,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|████████▎         | 25/54 [00:06<00:07,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████████▋         | 26/54 [00:06<00:07,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████         | 27/54 [00:07<00:07,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████████▎        | 28/54 [00:07<00:06,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|█████████▋        | 29/54 [00:07<00:06,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 30/54 [00:08<00:06,  3.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|██████████▎       | 31/54 [00:08<00:06,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|██████████▋       | 32/54 [00:08<00:05,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 33/54 [00:08<00:05,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  63%|███████████▎      | 34/54 [00:09<00:05,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|███████████▋      | 35/54 [00:09<00:05,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 36/54 [00:09<00:04,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|████████████▎     | 37/54 [00:09<00:04,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|████████████▋     | 38/54 [00:10<00:04,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 39/54 [00:10<00:04,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|█████████████▎    | 40/54 [00:10<00:03,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|█████████████▋    | 41/54 [00:10<00:03,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 42/54 [00:11<00:03,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|██████████████▎   | 43/54 [00:11<00:02,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%|██████████████▋   | 44/54 [00:11<00:02,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 45/54 [00:12<00:02,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|███████████████▎  | 46/54 [00:12<00:02,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  87%|███████████████▋  | 47/54 [00:12<00:01,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 48/54 [00:12<00:01,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|████████████████▎ | 49/54 [00:13<00:01,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|████████████████▋ | 50/54 [00:13<00:01,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 51/54 [00:13<00:00,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|█████████████████▎| 52/54 [00:13<00:00,  3.73it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|█████████████████▋| 53/54 [00:14<00:00,  3.74it/s]\u001b[A\n",
      "Epoch 0:  10%| | 530/5308 [10:36<1:35:36,  1.20s/it, v_num=hw72, train/loss=7.88\u001b[A\n",
      "Epoch 0:  17%|▏| 887/5308 [18:47<1:33:40,  1.27s/it, v_num=hw72, train/loss=7.87\u001b[A^C\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-enwiki-1k.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-small-4k Foundation (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-enwiki-4k/\" \\\n",
    "        --model.load_model=\"../model/L24-D2048-E{EMBED_SCALE_LABEL}-neox-v5base-init.pth\" \\\n",
    "        --model.position_loss_bias=0.1 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{FILENAME_PREFIX}-enwiki-4k/last.ckpt\" \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python3 dragon_test.py \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file '/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/position_loss_bias/../memory_script/eval_v5_memory_guided.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-enwiki-4k.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enwiki 10k / p1.0 - ctx 4k training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_LOSS_BIAS: 1.0\n",
      "WANDB_PREFIX: v5base-1B5-E0.1-P1.0\n",
      "FILENAME_PREFIX: v5base-1B5-E0_1-P1_0\n"
     ]
    }
   ],
   "source": [
    "POS_LOSS_BIAS=1.0\n",
    "POS_LOSS_BIAS_LABEL=str(POS_LOSS_BIAS).replace(\".\", \"_\")\n",
    "\n",
    "WANDB_PREFIX=f\"v5base-1B5-E{EMBED_SCALE}-P{POS_LOSS_BIAS}\"\n",
    "FILENAME_PREFIX=f\"v5base-1B5-E{EMBED_SCALE_LABEL}-P{POS_LOSS_BIAS_LABEL}\"\n",
    "\n",
    "print(\"POS_LOSS_BIAS:\", POS_LOSS_BIAS)\n",
    "print(\"WANDB_PREFIX:\", WANDB_PREFIX)\n",
    "print(\"FILENAME_PREFIX:\", FILENAME_PREFIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-18 08:35:03,020] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py:484: UserWarning: LightningCLI's args parameter is intended to run from within Python like if it were from the command line. To prevent mistakes it is not recommended to provide both args and command line arguments, got: sys.argv[1:]=['fit', '-c', '/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/position_loss_bias/v5base-enwiki-1k.yaml', '--trainer.logger.init_args.name=v5base-1B5-E0.1-P1.0 - Enwiki-small-4k Foundation (train-ctx=4k, deepspeed_stage_2_offload)', '--trainer.strategy=deepspeed_stage_2_offload', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5base-1B5-E0_1-P1_0-enwiki-4k/', '--model.load_model=../model/L24-D2048-E0_1-neox-v5base-init.pth', '--model.position_loss_bias=1.0', '--model.ctx_len=4096', '--model.bptt_learning_range=1'], args=['fit', '-c', '/home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/position_loss_bias/v5base-enwiki-1k.yaml', '--trainer.logger.init_args.name=v5base-1B5-E0.1-P1.0 - Enwiki-small-4k Foundation (train-ctx=4k, deepspeed_stage_2_offload)', '--trainer.strategy=deepspeed_stage_2_offload', '--trainer.devices=auto', '--trainer.callbacks.init_args.dirpath=../checkpoint/v5base-1B5-E0_1-P1_0-enwiki-4k/', '--model.load_model=../model/L24-D2048-E0_1-neox-v5base-init.pth', '--model.position_loss_bias=1.0', '--model.ctx_len=4096', '--model.bptt_learning_range=1'].\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 64432937\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 64432937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "cat: /sys/module/amdgpu/initstate: No such file or directory\n",
      "ERROR:root:Driver not initialized (amdgpu not found in modules)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230818_083505-14i30730\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mv5base-1B5-E0.1-P1.0 - Enwiki-small-4k Foundation (train-ctx=4k, deepspeed_stage_2_offload)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-5X-Experiments/runs/14i30730\u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       16\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             1\n",
      "   - accumulate_grad_batches: 16\n",
      "   - effective_batch_size:    16\n",
      "\n",
      "Saving the dataset (1/1 shards): 100%|█| 5308/5308 [00:00<00:00, 51259.81 exampl\n",
      "Saving the dataset (1/1 shards): 100%|█| 54/54 [00:00<00:00, 8914.57 examples/s]\n",
      "[rank: 0] Global seed set to 64432937\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-08-18 08:35:20,020] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "#\n",
      "# RWKV lighting_trainer.py important notes \n",
      "# https://github.com/RWKV/RWKV-infctx-trainer \n",
      "#\n",
      "# - Ensure your host is not running cuda 12.0 (use either 11.8, or >=12.1), as this is known to have freeze issues\n",
      "# - The terms used in wandb / the progress bar can be confusing, see the github README.md for beter clarifications\n",
      "# - When resuming from checkpoint, the estimated time is inaccurate\n",
      "#\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  6.000e-04 (0.0006)\n",
      "    - lr_final: 4.000e-04 (0.0004)\n",
      "\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3340160846710205 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1] and sizes[(1515106304, False), (1536, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.431 Total estimated model params size (MB)\n",
      "Epoch 0:   3%| | 155/5308 [02:50<1:34:19,  1.10s/it, v_num=0730, train/loss=8.12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%| | 157/5308 [02:51<1:33:54,  1.09s/it, v_num=0730, train/loss=8.94"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/v5base-enwiki-1k.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki-small-4k Foundation (train-ctx=4k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --trainer.callbacks.init_args.dirpath=\"../checkpoint/{FILENAME_PREFIX}-enwiki-4k/\" \\\n",
    "        --model.load_model=\"../model/L24-D2048-E{EMBED_SCALE_LABEL}-neox-v5base-init.pth\" \\\n",
    "        --model.position_loss_bias=1.0 \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/{FILENAME_PREFIX}-enwiki-4k/last.ckpt\" \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && \\\n",
    "    export RWKV_WAVENET_LAYERS=\"{RWKV_WAVENET_LAYERS}\" && \\\n",
    "    python3 dragon_test.py \"../model/{FILENAME_PREFIX}-enwiki-4k.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick memory test\n",
    "!python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/{FILENAME_PREFIX}-enwiki-4k.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
