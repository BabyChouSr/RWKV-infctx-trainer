{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TokenShift Memory benchmarking\n",
    "\n",
    "As current TokenShift models, have started to drastically surpass the raven model, this is a varient that focuses on such models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the size of the CSV data, we did not include it in the repository. You can download our current CSV data from hugging face\n",
    "!mkdir -p ./logs\n",
    "\n",
    "# Experimental V5 models\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/raw/main/experiment/memory-bench/logs/BaseV5-C-Tune5-1k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/BaseV5-C-Tune5-4k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/BaseV5-C-Tune5-16k.csv\"\n",
    "\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/raw/main/experiment/memory-bench/logs/v5-L6-D1024-E0_1-1k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L6-D1024-E0_1-4k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L6-D1024-E0_1-16k.csv\"\n",
    "\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/raw/main/experiment/memory-bench/logs/v5-L6-D2048-E0_1-1k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L6-D2048-E0_1-4k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L6-D2048-E0_1-16k.csv\"\n",
    "\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/raw/main/experiment/memory-bench/logs/v5-L6-D4096-E0_1-1k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L6-D4096-E0_1-4k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L6-D4096-E0_1-16k.csv\"\n",
    "\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/raw/main/experiment/memory-bench/logs/v5-L96-D1024-E0_1-mem-ctx-8k-1k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L96-D1024-E0_1-mem-ctx-8k-4k.csv\"\n",
    "!cd ./logs && wget -nc \"https://huggingface.co/rwkv-x-dev/rwkv-x-playground/resolve/main/experiment/memory-bench/logs/v5-L96-D1024-E0_1-mem-ctx-8k-16k.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required pip libraries\n",
    "!python -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Define a custom function to calculate averages for the first N elements\n",
    "def calculate_first_n_avg(n, s):\n",
    "    if(n == -1):\n",
    "        return s.iloc[:].mean()\n",
    "    return s.iloc[:n].mean()\n",
    "\n",
    "# Groupby the 'eval_token_count' and calculate the average for the first 5, 10, and 20 'eval_token_pos'\n",
    "def group_csv_data(inCSV, modelName):\n",
    "    grouped_data = inCSV.groupby(['eval_token_count', 'is_random_baseline']).apply(lambda x: pd.Series({\n",
    "        'First 1 tokens average': calculate_first_n_avg(1, x['eval_token_pos']),\n",
    "        'First 2 tokens average': calculate_first_n_avg(2, x['eval_token_pos']),\n",
    "        'First 5 tokens average': calculate_first_n_avg(5, x['eval_token_pos']),\n",
    "        'First 10 tokens average': calculate_first_n_avg(10, x['eval_token_pos']),\n",
    "        'First 25 tokens average': calculate_first_n_avg(25, x['eval_token_pos']),\n",
    "        'First 50 tokens average': calculate_first_n_avg(50, x['eval_token_pos']),\n",
    "        'First 100 tokens average': calculate_first_n_avg(100, x['eval_token_pos']),\n",
    "        'First 250 tokens average': calculate_first_n_avg(250, x['eval_token_pos']),\n",
    "        'First 500 tokens average': calculate_first_n_avg(500, x['eval_token_pos']),\n",
    "        'First 750 tokens average': calculate_first_n_avg(750, x['eval_token_pos']),\n",
    "        'First 1000 tokens average': calculate_first_n_avg(1000, x['eval_token_pos']),\n",
    "        'tokens average position': calculate_first_n_avg(-1, x['eval_token_pos']),\n",
    "        \"match_count\": x[\"matched\"].sum(),\n",
    "        \"match_percentage\": x[\"matched\"].sum() * 100.0 / x[\"matched\"].count() ,\n",
    "    })).reset_index()\n",
    "    grouped_data['model'] = modelName\n",
    "    return grouped_data\n",
    "\n",
    "# Read a CSV file, and group the data\n",
    "def group_csv_file(filepath, modelName):\n",
    "    return group_csv_data(pd.read_csv(filepath), modelName)\n",
    "\n",
    "# Merge the DataFrames \n",
    "full_grouped_data = pd.concat([\n",
    "\n",
    "    group_csv_file(\"./logs/BaseV5-C-Tune5-1k.csv\", \"V5r2-Baseline 1B5 (L24-D2048)\"),\n",
    "    group_csv_file(\"./logs/BaseV5-C-Tune5-4k.csv\", \"V5r2-Baseline 1B5 (L24-D2048)\"),\n",
    "    group_csv_file(\"./logs/BaseV5-C-Tune5-16k.csv\", \"V5r2-Baseline 1B5 (L24-D2048)\"),\n",
    "\n",
    "    group_csv_file(\"./logs/v5-L6-D1024-E0_1-1k.csv\", \"V5r2-L6-D1024\"),\n",
    "    group_csv_file(\"./logs/v5-L6-D1024-E0_1-4k.csv\", \"V5r2-L6-D1024\"),\n",
    "    group_csv_file(\"./logs/v5-L6-D1024-E0_1-16k.csv\", \"V5r2-L6-D1024\"),\n",
    "\n",
    "    group_csv_file(\"./logs/v5-L6-D2048-E0_1-1k.csv\", \"V5r2-L6-D2048\"),\n",
    "    group_csv_file(\"./logs/v5-L6-D2048-E0_1-4k.csv\", \"V5r2-L6-D2048\"),\n",
    "    group_csv_file(\"./logs/v5-L6-D2048-E0_1-16k.csv\", \"V5r2-L6-D2048\"),\n",
    "\n",
    "    group_csv_file(\"./logs/v5-L6-D4096-E0_1-1k.csv\", \"V5r2-L6-D4096\"),\n",
    "    group_csv_file(\"./logs/v5-L6-D4096-E0_1-4k.csv\", \"V5r2-L6-D4096\"),\n",
    "    group_csv_file(\"./logs/v5-L6-D4096-E0_1-16k.csv\", \"V5r2-L6-D4096\"),\n",
    "\n",
    "    group_csv_file(\"./logs/v5-L96-D1024-E0_1-mem-ctx-8k-1k.csv\", \"V5r2-L96-D1024\"),\n",
    "    group_csv_file(\"./logs/v5-L96-D1024-E0_1-mem-ctx-8k-4k.csv\", \"V5r2-L96-D1024\"),\n",
    "    group_csv_file(\"./logs/v5-L96-D1024-E0_1-mem-ctx-8k-16k.csv\", \"V5r2-L96-D1024\"),\n",
    "    \n",
    "])\n",
    "\n",
    "# Plot the data\n",
    "full_grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get the position values, of the average token prediction, withou ONLY the output.\n",
    "# this helps get the approximate \"random\" score baseline, while accounting for the fact that the model may eventually notice patterns that makes it not truely random\n",
    "# (eg. no special characters, etc), as the sample grows.\n",
    "\n",
    "# # Filter out for noise baseline\n",
    "random_baseline = full_grouped_data[full_grouped_data['is_random_baseline'] == True]\n",
    "\n",
    "# # Geet the average first 1000 tokens for all models\n",
    "# random_baseline_pos = random_baseline.groupby(['model']).mean()['First 1000 tokens average'][\"Raven 1B5\"]\n",
    "# half_random_base_line_pos = random_baseline_pos / 2\n",
    "\n",
    "# print(\"random_baseline_pos\", random_baseline_pos)\n",
    "# print(\"half_random_base_line_pos\", half_random_base_line_pos)\n",
    "\n",
    "# Get the key from one of the models\n",
    "key = random_baseline.groupby(['model']).mean()['First 1000 tokens average'].keys()[0]\n",
    "\n",
    "# Get the specialize model randomized baseline\n",
    "special_random_baseline_pos = random_baseline.groupby(['model']).mean()['First 1000 tokens average'][key]\n",
    "special_half_random_base_line_pos = special_random_baseline_pos / 2\n",
    "\n",
    "print(\"special_random_baseline_pos\", special_random_baseline_pos)\n",
    "print(\"special_half_random_base_line_pos\", special_half_random_base_line_pos)\n",
    "\n",
    "# Give the randomized baseline values\n",
    "random_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing function\n",
    "def plotGrapData(sizeArr=[2], redline=False):\n",
    "    # Lets join the size array, to a single str for logging\n",
    "    sizeStr = \", \".join([str(x) for x in sizeArr])\n",
    "\n",
    "    # Plot the axis\n",
    "    fig, ax = plt.subplots(figsize=(15,7)) #\n",
    "\n",
    "    # Get the highest value for sizeArr\n",
    "    max_size = max(sizeArr)\n",
    "\n",
    "    # Filter the data accordingly\n",
    "    filtered_data = full_grouped_data[full_grouped_data['is_random_baseline'] == False]\n",
    "    if max_size > 0:\n",
    "        filtered_data = filtered_data[filtered_data['eval_token_count'] <= max_size]\n",
    "\n",
    "    # Plot the data, excluding is random baseline = true\n",
    "    for key, grp in filtered_data.groupby('model'):\n",
    "        if len(sizeArr) == 1:\n",
    "            if(sizeArr[0] == -1):\n",
    "                ax = grp.plot(\n",
    "                    ax=ax, kind='line', x='eval_token_count', \n",
    "                    y=f'tokens average position',\n",
    "                    \n",
    "                    label=f'{key}'\n",
    "                )\n",
    "            else:\n",
    "                ax = grp.plot(\n",
    "                    ax=ax, kind='line', x='eval_token_count', \n",
    "                    y=f'First {sizeArr[0]} tokens average', \n",
    "                    label=f'{key}'\n",
    "                )\n",
    "        else:\n",
    "            ax = grp.plot(\n",
    "                ax=ax, kind='line', x='eval_token_count', \n",
    "                y=f'First {sizeArr[0]} tokens average', \n",
    "                label=f'{key} - First {sizeArr[0]} tokens average'\n",
    "            )\n",
    "\n",
    "        if len(sizeArr) >= 2:\n",
    "            ax = grp.plot(\n",
    "                ax=ax, kind='line', x='eval_token_count',\n",
    "                y=f'First {sizeArr[1]} tokens average', \n",
    "                label=f'{key} - First {sizeArr[1]} tokens average', linestyle='dashed'\n",
    "            )\n",
    "\n",
    "        if len(sizeArr) >= 3:\n",
    "            ax = grp.plot(ax=ax, kind='line', x='eval_token_count', \n",
    "                          y=f'First {sizeArr[2]} tokens average', \n",
    "                          label=f'{key} - First {sizeArr[2]} tokens average', linestyle='dotted'\n",
    "            )\n",
    "\n",
    "    # # Limit the X axis to max_size\n",
    "    # if max_size > 0:\n",
    "    #     ax.set_xlim(0, max_size)\n",
    "\n",
    "    # Add redline if set\n",
    "    if redline != False:\n",
    "        # ax.axhline(y=half_random_base_line_pos, color='r', linestyle='-.', label='50 percent of Raven Randomized baseline')\n",
    "        ax.axhline(y=special_half_random_base_line_pos, color='r', linestyle=':', label='50 percent of Specialized Model Randomized baseline')\n",
    "\n",
    "    # Title overwrites?\n",
    "    if( sizeArr[0] == -1):\n",
    "        ax.set_title(f'Average position of all tokens in long sequence')\n",
    "        ax.set_ylabel(f'Average position of all the correct tokens, in sorted probability order')\n",
    "    else:\n",
    "        ax.set_title(f'Recall of the first {sizeStr} tokens in long sequence')\n",
    "        ax.set_ylabel(f'Average position of first {sizeStr} correct tokens, in sorted probability order')\n",
    "    ax.set_xlabel(f'Prompt Length (tokens, used in long sequence)')\n",
    "\n",
    "    # Include grid lines - with major, and minor grid\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "    ax.grid(which='minor', linestyle=':', linewidth='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highscores for each models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets extract all the \"high score\" for each model\n",
    "df = full_grouped_data\n",
    "\n",
    "models = df.model.unique()\n",
    "\n",
    "results = []\n",
    "for model in models:\n",
    "    model_df = df[df.model == model]\n",
    "    \n",
    "    # Last highest match_percentage and associated metrics\n",
    "    match_percs = model_df.match_percentage.sort_values(ascending=False)\n",
    "    max_match_perc = match_percs.values[0]\n",
    "    max_match_perc_row = model_df[model_df.match_percentage == max_match_perc]\n",
    "\n",
    "    # Last highest match_count and associated metrics\n",
    "    match_counts = model_df.match_count.sort_values(ascending=False)\n",
    "    max_match_count = match_counts.values[0]\n",
    "    max_match_count_row = model_df[model_df.match_count == max_match_count]\n",
    "\n",
    "    # Last match_percentage >= 90 and associated metrics\n",
    "    flipped_model_df = model_df.sort_values(by=['eval_token_count'], ascending=False)\n",
    "    for idx, row in flipped_model_df.iterrows():\n",
    "        if row['match_percentage'] >= 90.0:\n",
    "            match_90_row = row\n",
    "            break\n",
    "            \n",
    "    results.append({\n",
    "        'model': model,\n",
    "\n",
    "        'max%': max_match_perc,\n",
    "        'max% : input': int(max_match_perc_row.eval_token_count.values[-1]),\n",
    "\n",
    "        '90% match: input': match_90_row.eval_token_count,\n",
    "        '90% match: match': match_90_row.match_count,\n",
    "        '90% match: %': match_90_row.match_percentage,\n",
    "\n",
    "        'matched: input': int(max_match_count_row.eval_token_count.values[0]),\n",
    "        'matched: count': int(max_match_count),\n",
    "        'matched: %': max_match_count_row.match_percentage.values[0]\n",
    "\n",
    "    }) \n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Plotting of high level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets chart too much data at too many points, so we can get a better idea of the trend\n",
    "# # before narrowing it down, commented out, unless you really want it\n",
    "# plotGrapData([1,2])\n",
    "# plotGrapData([5,10,25])\n",
    "# plotGrapData([50,100,250])\n",
    "# plotGrapData([500,750,1000], redline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets focus on first 1k\n",
    "plotGrapData([1000]) #, redline=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets chart overall average position\n",
    "plotGrapData([-1]) #, redline=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
