{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV v5 Baseline 1B5\n",
    "This model is based on the RWKV standard 1B5 model\n",
    "\n",
    "- 24 layers\n",
    "- 2048 embedding size\n",
    "\n",
    "Going through the modified memory training for v5 models\n",
    "\n",
    "**Note:** This project assumes you have the rwkv-infctx conda env setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets setup the various directories, and init the model\n",
    "!mkdir -p ../../../../model/\n",
    "!mkdir -p ../../../../datapath/\n",
    "!mkdir -p ../../../../checkpoint/\n",
    "\n",
    "# Init the model\n",
    "!cd ../../../../RWKV-v5 && python3 ./init_model.py --n_layer 24 --n_embd 2048 --vocab_size neox --skip-if-exists ../model/L24-D2048-neox-v5-init.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/notebook/experiment/rwkv-x-exp/V5-Base-1B5\n",
      "INFERENCE_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5\n",
      "TRAINER_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5\n",
      "PROJECT_DIR: /home/ubuntu/rwkv5x-tokenshift-exp-A\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"V5-Base-1B5-C\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "INFERENCE_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v5/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"INFERENCE_DIR:\", INFERENCE_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 : Foundation model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 5.26k/5.26k [00:00<00:00, 9.37MB/s]\n",
      "Downloading and preparing dataset json/RyokoExtra--SuperWIKI-Cleaned to /home/ubuntu/.cache/huggingface/datasets/RyokoExtra___json/RyokoExtra--SuperWIKI-Cleaned-abb930ec61eb61a3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                              | 0.00/442M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   3%|▋                     | 14.2M/442M [00:00<00:03, 142MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 28.4M/442M [00:00<00:08, 50.4MB/s]\u001b[A\n",
      "Downloading data:   8%|█▋                   | 36.3M/442M [00:00<00:08, 45.4MB/s]\u001b[A\n",
      "Downloading data:  11%|██▎                  | 48.0M/442M [00:00<00:07, 50.8MB/s]\u001b[A\n",
      "Downloading data:  14%|██▉                  | 62.1M/442M [00:01<00:06, 59.2MB/s]\u001b[A\n",
      "Downloading data:  16%|███▎                 | 68.8M/442M [00:01<00:07, 46.9MB/s]\u001b[A\n",
      "Downloading data:  18%|███▊                 | 79.5M/442M [00:01<00:06, 56.1MB/s]\u001b[A\n",
      "Downloading data:  19%|████                 | 86.1M/442M [00:01<00:06, 51.3MB/s]\u001b[A\n",
      "Downloading data:  22%|████▌                | 96.0M/442M [00:01<00:06, 51.0MB/s]\u001b[A\n",
      "Downloading data:  23%|█████                 | 102M/442M [00:01<00:06, 53.2MB/s]\u001b[A\n",
      "Downloading data:  25%|█████▌                | 111M/442M [00:02<00:05, 59.1MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▊                | 118M/442M [00:02<00:07, 46.3MB/s]\u001b[A\n",
      "Downloading data:  29%|██████▎               | 128M/442M [00:02<00:07, 40.0MB/s]\u001b[A\n",
      "Downloading data:  32%|███████               | 143M/442M [00:02<00:05, 58.7MB/s]\u001b[A\n",
      "Downloading data:  34%|███████▌              | 151M/442M [00:03<00:06, 43.0MB/s]\u001b[A\n",
      "Downloading data:  36%|███████▉              | 160M/442M [00:03<00:05, 47.1MB/s]\u001b[A\n",
      "Downloading data:  39%|████████▌             | 173M/442M [00:03<00:04, 61.1MB/s]\u001b[A\n",
      "Downloading data:  41%|████████▉             | 181M/442M [00:03<00:06, 41.5MB/s]\u001b[A\n",
      "Downloading data:  43%|█████████▌            | 191M/442M [00:03<00:05, 49.6MB/s]\u001b[A\n",
      "Downloading data:  45%|█████████▊            | 198M/442M [00:03<00:05, 46.9MB/s]\u001b[A\n",
      "Downloading data:  47%|██████████▏           | 206M/442M [00:04<00:05, 43.7MB/s]\u001b[A\n",
      "Downloading data:  48%|██████████▌           | 211M/442M [00:04<00:05, 41.8MB/s]\u001b[A\n",
      "Downloading data:  51%|███████████▏          | 224M/442M [00:04<00:04, 49.6MB/s]\u001b[A\n",
      "Downloading data:  54%|███████████▊          | 238M/442M [00:04<00:03, 65.8MB/s]\u001b[A\n",
      "Downloading data:  56%|████████████▏         | 246M/442M [00:04<00:04, 48.1MB/s]\u001b[A\n",
      "Downloading data:  58%|████████████▋         | 255M/442M [00:05<00:03, 54.0MB/s]\u001b[A\n",
      "Downloading data:  59%|█████████████         | 262M/442M [00:05<00:03, 51.5MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████▌        | 272M/442M [00:05<00:03, 51.7MB/s]\u001b[A\n",
      "Downloading data:  65%|██████████████▏       | 286M/442M [00:05<00:02, 68.8MB/s]\u001b[A\n",
      "Downloading data:  66%|██████████████▌       | 294M/442M [00:05<00:03, 39.0MB/s]\u001b[A\n",
      "Downloading data:  69%|███████████████       | 304M/442M [00:06<00:02, 47.2MB/s]\u001b[A\n",
      "Downloading data:  70%|███████████████▍      | 311M/442M [00:06<00:02, 47.0MB/s]\u001b[A\n",
      "Downloading data:  72%|███████████████▉      | 320M/442M [00:06<00:02, 47.8MB/s]\u001b[A\n",
      "Downloading data:  76%|████████████████▋     | 335M/442M [00:06<00:01, 66.8MB/s]\u001b[A\n",
      "Downloading data:  78%|█████████████████     | 344M/442M [00:06<00:02, 49.1MB/s]\u001b[A\n",
      "Downloading data:  80%|█████████████████▌    | 352M/442M [00:07<00:02, 43.6MB/s]\u001b[A\n",
      "Downloading data:  83%|██████████████████▏   | 367M/442M [00:07<00:01, 61.0MB/s]\u001b[A\n",
      "Downloading data:  85%|██████████████████▋   | 376M/442M [00:07<00:01, 51.6MB/s]\u001b[A\n",
      "Downloading data:  87%|███████████████████   | 384M/442M [00:07<00:01, 57.1MB/s]\u001b[A\n",
      "Downloading data:  88%|███████████████████▍  | 391M/442M [00:07<00:00, 56.1MB/s]\u001b[A\n",
      "Downloading data:  90%|███████████████████▉  | 400M/442M [00:07<00:00, 49.3MB/s]\u001b[A\n",
      "Downloading data:  94%|████████████████████▌ | 414M/442M [00:07<00:00, 66.6MB/s]\u001b[A\n",
      "Downloading data:  95%|████████████████████▉ | 422M/442M [00:08<00:00, 61.3MB/s]\u001b[A\n",
      "Downloading data:  98%|█████████████████████▍| 432M/442M [00:08<00:00, 60.0MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 442M/442M [00:08<00:00, 52.2MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/31.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  37%|███████▋             | 11.4M/31.1M [00:00<00:00, 114MB/s]\u001b[A\n",
      "Downloading data:  73%|██████████████▋     | 22.8M/31.1M [00:00<00:00, 68.4MB/s]\u001b[A\n",
      "Downloading data: 100%|████████████████████| 31.1M/31.1M [00:00<00:00, 58.7MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:09<00:00,  9.97s/it]\n",
      "Extracting data files: 100%|██████████████████████| 1/1 [00:07<00:00,  7.39s/it]\n",
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/RyokoExtra___json/RyokoExtra--SuperWIKI-Cleaned-abb930ec61eb61a3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 136.80it/s]\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5/preload_datapath.py\", line 37, in <module>\n",
      "    dataMod.prepare_data()\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5/src/data.py\", line 458, in prepare_data\n",
      "    prepare_data_static(**self._init_locals)\n",
      "  File \"/home/ubuntu/rwkv5x-tokenshift-exp-A/RWKV-v5/src/data.py\", line 301, in prepare_data_static\n",
      "    dataset_features = src_dataset[\"train\"].features\n",
      "                       ~~~~~~~~~~~^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/datasets/dataset_dict.py\", line 57, in __getitem__\n",
      "    return super().__getitem__(k)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'train'\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/V5-Base-1B5-enwiki.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/V5-Base-1B5-enwiki.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Enwiki Foundation (train-ctx=16k, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\" \\\n",
    "        --model.ctx_len=4096 \\\n",
    "        --model.bptt_learning_range=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/V5-Base-1B5-enwiki/last.ckpt\" \"../model/V5-Base-1B5-Stage1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/V5-Base-1B5-Stage1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py ../model/V5-Base-1B5-Stage1.pth \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/V5-Base-1B5-Stage1.pth\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 : Instruct Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets preload the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_datapath.py \"{NOTEBOOK_DIR}/V5-Base-1B5-instruct.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the instruct finetuning\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python lightning_trainer.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/V5-Base-1B5-instruct.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Instruct (train-ctx=4096, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \"../checkpoint/V5-Base-1B5-instruct/last.ckpt\" \"../model/V5-Base-1B5-Stage2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh \"../model/V5-Base-1B5-Stage2.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{INFERENCE_DIR}\" && python3 dragon_test.py \"../model/V5-Base-1B5-Stage2.pth\" \"cuda fp32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do a quick memory test\n",
    "# # (We dun expect this to work, as we have not finetune for memory recall, but its a baseline)\n",
    "# !python3 ../memory_script/eval_v5_memory_guided.py \"{PROJECT_DIR}/model/V5-Base-1B5-Stage2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
