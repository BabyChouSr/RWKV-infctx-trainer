{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-B 1B4 (Memory Finetune)\n",
    "This continues off from `Echo-B-1B4-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "This is done generally in 3 Tune stages\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set.\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens.\n",
    "- Tune 3: Mid ctx size (1024), stage 2, scaled up to 1024 context sizes.\n",
    "\n",
    "In all cases, the input tokens is always masked. And we intentionally use the limited word set for memory training, which matches the same wordset used in the original memory evaluation of raven pretrained models. This is intentional to serve as both consistent comparision between experiments, and resonable training time.\n",
    "\n",
    "One of the issue faced previously with an excessive large word set, is that the model would be required to see \"new words\" atleast a few time before being able to train the memory process. This drastically slowed down the process as the large word list meant the model was constantly spending time learning new words (instead of memory training).\n",
    "\n",
    "If we want to increase the number / type of words the model can handle for memory training, that can be done later as a stage 4 memory tune if needed. But that exceeds the current requirements for the memory experiment process.\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Download the pretrained model\n",
    "(if you want to skip the the basemodel train + instruct tune)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init required dirs\n",
    "# !mkdir -p ../../../model/\n",
    "# !mkdir -p ../../../datapath/\n",
    "# !mkdir -p ../../../checkpoint/\n",
    "\n",
    "# # Download the Stage2.pth file\n",
    "# !rm -rf ../../../model/Echo-B-1B4-Stage2.pth\n",
    "# !cd ../../../model/ && wget https://huggingface.co/picocreator/memory-size-experiment-for-rwkv/resolve/main/Echo-B-1B4-Stage2.pth\n",
    "# !ls -alh ../../../model/Echo-B-1B4-Stage2.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your environment settings\n",
    "(!Important: you will need to rerun the below cell, if you restart your kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_1\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory-enwiki-v2\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo\n",
      "PROJECT_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_1\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"(8x3090) Echo-B-1B4\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(PROJECT_DIR, \"./RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 1 : Simple Memory instruct finetuning\n",
    "\n",
    "- Tune 1: Low ctx size (512), Training with only the input masked. This does very limited memory training, and is used primarily to train the instruction set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 10 max words, 2500 samples - at ./dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 15 max words, 2500 samples - at ./dataset/word-15-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 2500 samples - at ./dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 2 max words, 5000 samples - at ./dataset/word-2-count.jsonl\n",
      "Generated JSONL file with - 25 max words, 2500 samples - at ./dataset/word-25-count.jsonl\n",
      "Generated JSONL file with - 5 max words, 5000 samples - at ./dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 2500 samples - at ./dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 50 max words, 2500 samples - at ./dataset/word-50-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ./dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 2500 samples - at ./dataset/word-60-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 2500 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 2500 samples - at ./dataset/word-200-count.jsonl\n",
      "## Done ##\n",
      "total 21M\n",
      "drwxr-xr-x 2 root root  330 Jul 14 04:44 .\n",
      "drwxr-xr-x 5 root root 4.0K Jul 14 04:44 ..\n",
      "-rw-r--r-- 1 root root 614K Jul 14 04:44 word-10-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.8M Jul 14 04:44 word-100-count.jsonl\n",
      "-rw-r--r-- 1 root root 719K Jul 14 04:44 word-15-count.jsonl\n",
      "-rw-r--r-- 1 root root 836K Jul 14 04:44 word-2-count.jsonl\n",
      "-rw-r--r-- 1 root root 855K Jul 14 04:44 word-20-count.jsonl\n",
      "-rw-r--r-- 1 root root 5.2M Jul 14 04:44 word-200-count.jsonl\n",
      "-rw-r--r-- 1 root root 969K Jul 14 04:44 word-25-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.4M Jul 14 04:44 word-40-count.jsonl\n",
      "-rw-r--r-- 1 root root 967K Jul 14 04:44 word-5-count.jsonl\n",
      "-rw-r--r-- 1 root root 1.6M Jul 14 04:44 word-50-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 14 04:44 word-60-count.jsonl\n",
      "-rw-r--r-- 1 root root 2.3M Jul 14 04:44 word-80-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function. \n",
    "#\n",
    "# Note that all document samples, are randomized between the target word count, \n",
    "# to half of the target word count.\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-5-count.jsonl  5  5000 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-10-count.jsonl 10 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-15-count.jsonl 15 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-20-count.jsonl 20 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-25-count.jsonl 25 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-40-count.jsonl 40 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-50-count.jsonl 50 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-60-count.jsonl 80 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-80-count.jsonl 80 2500 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-100-count.jsonl 100 2500 &\n",
    "python ./memory_script/gen_limited_segmented_jsonl.py ./dataset/word-200-count.jsonl 200 2500 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6482.70it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 342.50it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 200.77it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-1.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-B-1B4-mem-finetune-1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 4190240465\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 4190240465\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230714_044445-5f9aziq0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/5f9aziq0\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=512 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    6144 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_512_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.11/dist-packages/torch/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.11/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.11/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=512 -c /root/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/usr/local/lib/python3.11/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_512_bf16.so\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 353.77it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bb872585ce30b392_*_of_00064.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-0960727d51f49ca1/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-f6b9ea10bac7065c_*_of_00064.arrow\n",
      "[rank: 0] Global seed set to 4190240465                                         \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "[2023-07-14 04:45:15,728] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[rank: 3] Global seed set to 4190240465\n",
      "[rank: 1] Global seed set to 4190240465\n",
      "[rank: 7] Global seed set to 4190240465\n",
      "[rank: 6] Global seed set to 4190240465\n",
      "[rank: 4] Global seed set to 4190240465\n",
      "[rank: 2] Global seed set to 4190240465\n",
      "[rank: 5] Global seed set to 4190240465\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "[rank: 3] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[2023-07-14 04:45:43,431] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 1] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[2023-07-14 04:45:51,318] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 7] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[2023-07-14 04:45:51,471] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 6] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[2023-07-14 04:45:51,773] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 2] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "[2023-07-14 04:45:51,837] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 4] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[2023-07-14 04:45:51,860] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[rank: 5] Global seed set to 4190240465\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[2023-07-14 04:45:51,868] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "[RWKV.model] Configuring optimizer with\n",
      "    - lr_init:  8.000e-04 (0.0008)\n",
      "    - lr_final: 5.000e-04 (0.0005)\n",
      "\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0640418529510498 seconds\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10123467445373535 seconds\n",
      "Time to load fused_adam op: 0.10140156745910645 seconds\n",
      "Time to load fused_adam op: 0.1013326644897461 seconds\n",
      "Time to load fused_adam op: 0.1013331413269043 seconds\n",
      "Time to load fused_adam op: 0.10151958465576172 seconds\n",
      "Time to load fused_adam op: 0.10189390182495117 seconds\n",
      "Time to load fused_adam op: 0.10189986228942871 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.06637144088745117 seconds\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10195732116699219 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1018683910369873 seconds\n",
      "Time to load utils op: 0.10200166702270508 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1017305850982666 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10190486907958984 seconds\n",
      "Time to load utils op: 0.10180854797363281 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10186219215393066 seconds\n",
      "Rank: 7 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 5 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 2 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 0 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 6 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 4 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 1 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Rank: 3 partition count [8, 8, 8] and sizes[(176559872, False), (12288, False), (12288, False)] \n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Time to load utils op: 0.0002732276916503906 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0003490447998046875 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.00028014183044433594 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002765655517578125 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002837181091308594 seconds\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0002808570861816406 seconds\n",
      "Time to load utils op: 0.0002779960632324219 seconds\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module utils, skipping build step...\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0005145072937011719 seconds\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 51.5 M\n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 2.0 K \n",
      "3 | head   | Linear     | 51.5 M\n",
      "--------------------------------------\n",
      "1.4 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 B     Total params\n",
      "5,650.702 Total estimated model params size (MB)\n",
      "Epoch 0:  18%|▏| 800/4371 [07:09<31:57,  1.86it/s, v_num=ziq0, train/loss=4.060]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|█| 4371/4371 [39:59<00:00,  1.82it/s, v_num=ziq0, train/loss=0.479\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                            | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|████                | 1/5 [00:00<00:00,  4.31it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|████████            | 2/5 [00:00<00:00,  4.61it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|████████████        | 3/5 [00:00<00:00,  4.71it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|████████████████    | 4/5 [00:00<00:00,  4.77it/s]\u001b[A\n",
      "Epoch 0: 100%|█| 4371/4371 [40:08<00:00,  1.82it/s, v_num=ziq0, train/loss=0.479\u001b[A\n",
      "Epoch 0: 100%|█| 4371/4371 [40:08<00:00,  1.82it/s, v_num=ziq0, train/loss=0.479\u001b[A`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Epoch 0: 100%|█| 4371/4371 [40:21<00:00,  1.81it/s, v_num=ziq0, train/loss=0.479\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▃▁▄▂▂▁▁▁▂██▂▁▂▁▆▂▂▄▄▃▁▁▁█▂▂▁▂▇▃▁▃▁▁▃▄▄▄▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▇█▃▆▅▆▃▆▄▅▇▂▃▅▆▂▂▅▁▇▁▆▇▆▅▁▁▇▂▁▂▅▁▂▆▂▄▅▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 36\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 2.04688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0005\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       validation/loss 2.72207\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-1 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/5f9aziq0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230714_044445-5f9aziq0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-1.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Processing zero checkpoint '../checkpoint/Echo-B-1B4-mem-finetune-1/last.ckpt/checkpoint'\n",
      "Detected checkpoint of type zero stage ZeroStageEnum.optimizer_states, world_size: 8\n",
      "Parsing checkpoint created by deepspeed==0.9.3\n",
      "Reconstructed fp32 state dict with 1734 params 1412675584 elements\n",
      "Saving fp32 state dict to ../model/Echo-B-1B4-Tune1.pth\n",
      "-rw-r--r-- 1 root root 5.3G Jul 14 05:27 ../model/Echo-B-1B4-Tune1.pth\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-B-1B4-mem-finetune-1/last.ckpt\" \\\n",
    "        \"../model/Echo-B-1B4-Tune1.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-B-1B4-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-B-1B4-Tune1.pth ...\n",
      "Strategy: (total 96+1=97 layers)\n",
      "* cuda [float32, float32], store 97 layers\n",
      "0-cuda-float32-float32 1-cuda-float32-float32 2-cuda-float32-float32 3-cuda-float32-float32 4-cuda-float32-float32 5-cuda-float32-float32 6-cuda-float32-float32 7-cuda-float32-float32 8-cuda-float32-float32 9-cuda-float32-float32 10-cuda-float32-float32 11-cuda-float32-float32 12-cuda-float32-float32 13-cuda-float32-float32 14-cuda-float32-float32 15-cuda-float32-float32 16-cuda-float32-float32 17-cuda-float32-float32 18-cuda-float32-float32 19-cuda-float32-float32 20-cuda-float32-float32 21-cuda-float32-float32 22-cuda-float32-float32 23-cuda-float32-float32 24-cuda-float32-float32 25-cuda-float32-float32 26-cuda-float32-float32 27-cuda-float32-float32 28-cuda-float32-float32 29-cuda-float32-float32 30-cuda-float32-float32 31-cuda-float32-float32 32-cuda-float32-float32 33-cuda-float32-float32 34-cuda-float32-float32 35-cuda-float32-float32 36-cuda-float32-float32 37-cuda-float32-float32 38-cuda-float32-float32 39-cuda-float32-float32 40-cuda-float32-float32 41-cuda-float32-float32 42-cuda-float32-float32 43-cuda-float32-float32 44-cuda-float32-float32 45-cuda-float32-float32 46-cuda-float32-float32 47-cuda-float32-float32 48-cuda-float32-float32 49-cuda-float32-float32 50-cuda-float32-float32 51-cuda-float32-float32 52-cuda-float32-float32 53-cuda-float32-float32 54-cuda-float32-float32 55-cuda-float32-float32 56-cuda-float32-float32 57-cuda-float32-float32 58-cuda-float32-float32 59-cuda-float32-float32 60-cuda-float32-float32 61-cuda-float32-float32 62-cuda-float32-float32 63-cuda-float32-float32 64-cuda-float32-float32 65-cuda-float32-float32 66-cuda-float32-float32 67-cuda-float32-float32 68-cuda-float32-float32 69-cuda-float32-float32 70-cuda-float32-float32 71-cuda-float32-float32 72-cuda-float32-float32 73-cuda-float32-float32 74-cuda-float32-float32 75-cuda-float32-float32 76-cuda-float32-float32 77-cuda-float32-float32 78-cuda-float32-float32 79-cuda-float32-float32 80-cuda-float32-float32 81-cuda-float32-float32 82-cuda-float32-float32 83-cuda-float32-float32 84-cuda-float32-float32 85-cuda-float32-float32 86-cuda-float32-float32 87-cuda-float32-float32 88-cuda-float32-float32 89-cuda-float32-float32 90-cuda-float32-float32 91-cuda-float32-float32 92-cuda-float32-float32 93-cuda-float32-float32 94-cuda-float32-float32 95-cuda-float32-float32 96-cuda-float32-float32 \n",
      "blocks.0.att.key.weight           f32   cuda:0   1024  1024 \n",
      "blocks.0.att.output.weight        f32   cuda:0   1024  1024 \n",
      "blocks.0.att.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.att.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.att.time_mix_v           f32   cuda:0   1024       \n",
      "blocks.0.att.value.weight         f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.key.weight           f32   cuda:0   1024  4096 \n",
      "blocks.0.ffn.receptance.weight    f32   cuda:0   1024  1024 \n",
      "blocks.0.ffn.time_mix_k           f32   cuda:0   1024       \n",
      "blocks.0.ffn.time_mix_r           f32   cuda:0   1024       \n",
      "blocks.0.ffn.value.weight         f32   cuda:0   4096  1024 \n",
      "blocks.0.ln1.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln1.weight               f32   cuda:0   1024       \n",
      "blocks.0.ln2.bias                 f32   cuda:0   1024       \n",
      "blocks.0.ln2.weight               f32   cuda:0   1024       \n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.95.att.key.weight          f32   cuda:0   1024  1024 \n",
      "blocks.95.att.output.weight       f32   cuda:0   1024  1024 \n",
      "blocks.95.att.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.att.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.att.time_mix_v          f32   cuda:0   1024       \n",
      "blocks.95.att.value.weight        f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.key.weight          f32   cuda:0   1024  4096 \n",
      "blocks.95.ffn.receptance.weight   f32   cuda:0   1024  1024 \n",
      "blocks.95.ffn.time_mix_k          f32   cuda:0   1024       \n",
      "blocks.95.ffn.time_mix_r          f32   cuda:0   1024       \n",
      "blocks.95.ffn.value.weight        f32   cuda:0   4096  1024 \n",
      "blocks.95.ln1.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln1.weight              f32   cuda:0   1024       \n",
      "blocks.95.ln2.bias                f32   cuda:0   1024       \n",
      "blocks.95.ln2.weight              f32   cuda:0   1024       \n",
      "emb.weight                        f32      cpu  50277  1024 \n",
      "head.weight                       f32   cuda:0   1024 50277 \n",
      "ln_out.bias                       f32   cuda:0   1024       \n",
      "ln_out.weight                     f32   cuda:0   1024       \n",
      "blocks.0.att.time_decay           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_decay          f32   cuda:0   1024       \n",
      "blocks.0.att.time_first           f32   cuda:0   1024       \n",
      "..............................................................................................\n",
      "blocks.95.att.time_first          f32   cuda:0   1024       \n",
      "###\n",
      "### Model validation start ###\n",
      "###\n",
      "Model validation at 5 tokens : 100.0% similarity, with 5 matched token, and 0 token mismatch\n",
      "Model validation at 10 tokens : 100.0% similarity, with 10 matched token, and 0 token mismatch\n",
      "Model validation at 15 tokens : 100.0% similarity, with 15 matched token, and 0 token mismatch\n",
      "Model validation at 20 tokens : 90.0% similarity, with 18 matched token, and 2 token mismatch\n",
      "Model validation at 25 tokens : 84.0% similarity, with 21 matched token, and 4 token mismatch\n",
      "Model validation at 30 tokens : 83.33333333333334% similarity, with 25 matched token, and 5 token mismatch\n",
      "Model validation at 35 tokens : 77.14285714285715% similarity, with 27 matched token, and 8 token mismatch\n",
      "Model validation at 40 tokens : 65.0% similarity, with 26 matched token, and 14 token mismatch\n",
      "Model validation at 45 tokens : 55.55555555555556% similarity, with 25 matched token, and 20 token mismatch\n",
      "Model validation at 50 tokens : 56.00000000000001% similarity, with 28 matched token, and 22 token mismatch\n",
      "Model validation at 55 tokens : 50.90909090909091% similarity, with 28 matched token, and 27 token mismatch\n",
      "Model validation at 60 tokens : 46.666666666666664% similarity, with 28 matched token, and 32 token mismatch\n",
      "Model validation at 65 tokens : 46.15384615384615% similarity, with 30 matched token, and 35 token mismatch\n",
      "Model validation at 70 tokens : 41.42857142857143% similarity, with 29 matched token, and 41 token mismatch\n",
      "Model validation at 75 tokens : 42.66666666666667% similarity, with 32 matched token, and 43 token mismatch\n",
      "Model validation at 80 tokens : 37.5% similarity, with 30 matched token, and 50 token mismatch\n",
      "Model validation at 85 tokens : 30.58823529411765% similarity, with 26 matched token, and 59 token mismatch\n",
      "Model validation at 90 tokens : 26.666666666666668% similarity, with 24 matched token, and 66 token mismatch\n",
      "Model validation at 95 tokens : 25.263157894736842% similarity, with 24 matched token, and 71 token mismatch\n",
      "Model validation at 100 tokens : 25.0% similarity, with 25 matched token, and 75 token mismatch\n",
      "Model validation at 110 tokens : 21.818181818181817% similarity, with 24 matched token, and 86 token mismatch\n",
      "Model validation at 120 tokens : 19.166666666666668% similarity, with 23 matched token, and 97 token mismatch\n",
      "Model validation at 130 tokens : 16.153846153846153% similarity, with 21 matched token, and 109 token mismatch\n",
      "Model validation at 140 tokens : 15.0% similarity, with 21 matched token, and 119 token mismatch\n",
      "Model validation at 150 tokens : 15.333333333333332% similarity, with 23 matched token, and 127 token mismatch\n",
      "Model validation at 175 tokens : 13.142857142857142% similarity, with 23 matched token, and 152 token mismatch\n",
      "Model validation at 200 tokens : 10.5% similarity, with 21 matched token, and 179 token mismatch\n",
      "Model validation at 225 tokens : 9.777777777777779% similarity, with 22 matched token, and 203 token mismatch\n",
      "Model validation at 250 tokens : 8.799999999999999% similarity, with 22 matched token, and 228 token mismatch\n",
      "Model validation at 275 tokens : 7.636363636363637% similarity, with 21 matched token, and 254 token mismatch\n",
      "Model validation at 300 tokens : 7.666666666666666% similarity, with 23 matched token, and 277 token mismatch\n",
      "Model validation at 325 tokens : 6.153846153846154% similarity, with 20 matched token, and 305 token mismatch\n",
      "Model validation at 350 tokens : 5.714285714285714% similarity, with 20 matched token, and 330 token mismatch\n",
      "Model validation at 375 tokens : 5.6000000000000005% similarity, with 21 matched token, and 354 token mismatch\n",
      "Model validation at 400 tokens : 5.25% similarity, with 21 matched token, and 379 token mismatch\n",
      "Model validation at 425 tokens : 4.705882352941177% similarity, with 20 matched token, and 405 token mismatch\n",
      "Model validation at 450 tokens : 4.444444444444445% similarity, with 20 matched token, and 430 token mismatch\n",
      "Model validation at 475 tokens : 4.2105263157894735% similarity, with 20 matched token, and 455 token mismatch\n",
      "Model validation at 500 tokens : 4.0% similarity, with 20 matched token, and 480 token mismatch\n",
      "Model validation at 550 tokens : 4.0% similarity, with 22 matched token, and 528 token mismatch\n",
      "Model validation at 600 tokens : 3.5000000000000004% similarity, with 21 matched token, and 579 token mismatch\n",
      "Model validation at 650 tokens : 3.3846153846153846% similarity, with 22 matched token, and 628 token mismatch\n",
      "Model validation at 700 tokens : 3.1428571428571432% similarity, with 22 matched token, and 678 token mismatch\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval\n",
    "#\n",
    "# Note that the expected performance \"is not that great\", as the model seems to be only loosely\n",
    "# learning the memorization task, and the instruction propmt. And is seem to be acting more\n",
    "# like an RNG based on the instruct. (Instead of the actual memorization task)\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Tune1.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune 2 : Low ctx size (512), memory training\n",
    "\n",
    "- Tune 2: Low ctx size (512), Training with instruction & input masked. This forces the actual memory training on the output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated a single JSONL file with 673 samples (50 token repeat) - 200 max words - at ./dataset/shuffle-word-200-count.jsonl\n",
      "Generated a single JSONL file with 1768 samples (50 token repeat) - 75 max words - at ./dataset/shuffle-word-75-count.jsonl\n",
      "Generated a single JSONL file with 1324 samples (50 token repeat) - 100 max words - at ./dataset/shuffle-word-100-count.jsonl\n",
      "Generated a single JSONL file with 2631 samples (50 token repeat) - 50 max words - at ./dataset/shuffle-word-50-count.jsonl\n",
      "## Done ##\n",
      "total 22K\n",
      "drwxrwxr-x 2 picocreator picocreator    6 Jul 14 14:21 .\n",
      "drwxrwxr-x 4 picocreator picocreator   12 Jul 14 14:16 ..\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.4M Jul 14 14:21 shuffle-word-100-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.4M Jul 14 14:21 shuffle-word-200-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.5M Jul 14 14:21 shuffle-word-50-count.jsonl\n",
      "-rw-rw-r-- 1 picocreator picocreator 1.5M Jul 14 14:21 shuffle-word-75-count.jsonl\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "########################################\n",
    "# Generate the required jsonl dataset\n",
    "########################################\n",
    "\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "#\n",
    "# We switch over to fully masked instruct+input, to properly learn the memorization task\n",
    "#\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-2-count.jsonl  2  5000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-5-count.jsonl  5  5000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-10-count.jsonl 10 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-15-count.jsonl 15 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-20-count.jsonl 20 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-25-count.jsonl 25 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-40-count.jsonl 40 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-50-count.jsonl 50 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-60-count.jsonl 80 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-80-count.jsonl 80 10000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-100-count.jsonl 100 5000 &\n",
    "python ./memory_script/gen_limited_prompt_completion_jsonl.py ./dataset/word-200-count.jsonl 200 5000 &\n",
    "\n",
    "#\n",
    "# We mixin the shuffled word list, so that we ensure all words / tokens are learned\n",
    "# however this might intrduce an exclusion bias (if seen this word, never repeat it), \n",
    "# so we limit the mixture of this data samples\n",
    "#\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-10-count.jsonl 10 20 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-15-count.jsonl 15 20 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-25-count.jsonl 25 30 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-50-count.jsonl 50 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-75-count.jsonl 75 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-100-count.jsonl 100 50 &\n",
    "python ./memory_script/shuffle_limited_prompt_completion_jsonl.py ./dataset/shuffle-word-200-count.jsonl 200 50 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\"\n",
    "\n",
    "ls -alh ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/picocreator/.cache/huggingface/datasets/json/default-2fdcb4be08d20e18/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|██████████████████| 1/1 [00:00<00:00, 13357.66it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 749.65it/s]\n",
      "Dataset json downloaded and prepared to /home/picocreator/.cache/huggingface/datasets/json/default-2fdcb4be08d20e18/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 320.84it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets pre tokenize the requried dataset\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-2.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-B-1B4-mem-finetune-2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 3772335697\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 3772335697\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230714_053206-50q6wti0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/50q6wti0\u001b[0m\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/fabric/connector.py:562: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "\n",
      "[RWKV.Trainer] Applying 'target_batch_size' with the following:\n",
      "   - target_batch_size:       256\n",
      "   - num_nodes:               1\n",
      "   - num_devices:             8\n",
      "   - accumulate_grad_batches: 32\n",
      "   - effective_batch_size:    256\n",
      "\n",
      "Resolving data files: 100%|██████████████████| 18/18 [00:00<00:00, 95445.60it/s]\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-d7d1e1c53e8b8fa4/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4198.50it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 226.52it/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "Generating train split: 5211 examples [00:02, 2309.05 examples/s]Failed to read file '/root/picocreator-memory-experiment/notebook/experiment/memory-enwiki-v2/dataset/shuffle-word-50-count.jsonl' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Invalid value. in row 201\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1+cu118'\n",
      "multiprocess.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/packaged_modules/json/json.py\", line 144, in _generate_tables\n",
      "    dataset = json.load(f)\n",
      "              ^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 701)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1879, in _prepare_split_single\n",
      "    for _, table in generator:\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/packaged_modules/json/json.py\", line 147, in _generate_tables\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/packaged_modules/json/json.py\", line 121, in _generate_tables\n",
      "    pa_table = paj.read_json(\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_json.pyx\", line 258, in pyarrow._json.read_json\n",
      "  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: JSON parse error: Invalid value. in row 201\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1912, in _prepare_split_single\n",
      "    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\n",
      "datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/picocreator-memory-experiment/RWKV-v4neo/new_train.py\", line 69, in <module>\n",
      "    cli_main()\n",
      "  File \"/root/picocreator-memory-experiment/RWKV-v4neo/new_train.py\", line 62, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 520, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 42, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 92, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 559, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 887, in _run\n",
      "    self._data_connector.prepare_data()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py\", line 94, in prepare_data\n",
      "    call._call_lightning_datamodule_hook(trainer, \"prepare_data\")\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 162, in _call_lightning_datamodule_hook\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/picocreator-memory-experiment/RWKV-v4neo/src/data.py\", line 330, in prepare_data\n",
      "    prepare_data_static(**self._init_locals)\n",
      "  File \"/root/picocreator-memory-experiment/RWKV-v4neo/src/data.py\", line 35, in prepare_data_static\n",
      "    src_dataset = load_dataset(**load_dataset_params)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1809, in load_dataset\n",
      "    builder_instance.download_and_prepare(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 909, in download_and_prepare\n",
      "    self._download_and_prepare(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1004, in _download_and_prepare\n",
      "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1796, in _prepare_split\n",
      "    for job_id, done, content in iflatmap_unordered(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 1354, in iflatmap_unordered\n",
      "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py\", line 1354, in <listcomp>\n",
      "    [async_result.get(timeout=0.05) for async_result in async_results]\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/multiprocess/pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "[rank: 6] Global seed set to 3772335697\n",
      "[rank: 3] Global seed set to 3772335697\n",
      "[rank: 5] Global seed set to 3772335697\n",
      "[rank: 2] Global seed set to 3772335697\n",
      "[rank: 1] Global seed set to 3772335697\n",
      "[rank: 4] Global seed set to 3772335697\n",
      "[rank: 7] Global seed set to 3772335697\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_512_bf16/build.ninja...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Building extension module wkv_512_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "Loading extension module wkv_512_bf16...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m(8x3090) Echo-B-1B4 - Mem-Finetune-2 (bs=256, train-ctx=512, deepspeed_stage_1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/50q6wti0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230714_053206-50q6wti0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the finetune model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export WANDB_MODE=\"{WANDB_MODE}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-B-1B4-mem-finetune-2.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-2 (bs=256, train-ctx=512, {DEEPSPEED_STRAT})\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-14 14:18:32,364] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/export_checkpoint.py\", line 623, in <module>\n",
      "    convert_zero_checkpoint_to_fp32_state_dict(args.checkpoint_dir, output_file)\n",
      "  File \"/home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/export_checkpoint.py\", line 537, in convert_zero_checkpoint_to_fp32_state_dict\n",
      "    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo/export_checkpoint.py\", line 516, in get_fp32_state_dict_from_zero_checkpoint\n",
      "    raise ValueError(f\"Unable to find 'latest' file at {latest_path}\")\n",
      "ValueError: Unable to find 'latest' file at ../checkpoint/Echo-B-1B4-mem-finetune-2/last.ckpt/latest\n",
      "ls: cannot access '../model/Echo-B-1B4-Tune1.pth': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python export_checkpoint.py \\\n",
    "        \"../checkpoint/Echo-B-1B4-mem-finetune-2/last.ckpt\" \\\n",
    "        \"../model/Echo-B-1B4-Tune2.pth\"\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ../model/Echo-B-1B4-Tune1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu118/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module wkv_cuda...\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 0\n",
      "\n",
      "Loading /root/picocreator-memory-experiment/model/Echo-B-1B4-Tune2.pth ...\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/picocreator-memory-experiment/notebook/experiment/memory-enwiki-v2/./memory_script/eval_memory_guided.py\", line 46, in <module>\n",
      "    model = RWKV(model=model_path, strategy=model_run_strat)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/jit/_script.py\", line 292, in init_then_script\n",
      "    original_init(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/rwkv/model.py\", line 104, in __init__\n",
      "    self.w = torch.load(args.MODEL_NAME, map_location='cpu') # load model to CPU first\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 791, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 271, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 252, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/picocreator-memory-experiment/model/Echo-B-1B4-Tune2.pth'\n"
     ]
    }
   ],
   "source": [
    "# Lets do a memory eval \n",
    "#\n",
    "# While not at its full potential, its memory ability should start emerging\n",
    "#\n",
    "!python3 ./memory_script/eval_memory_guided.py \"{PROJECT_DIR}/model/Echo-B-1B4-Tune2.pth\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
