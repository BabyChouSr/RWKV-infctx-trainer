{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory Finetune)\n",
    "This continues off from `Echo-A-1B5-basemodel.ipynb` to perform the full memory finetune & testing process\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[?25hm#########\u001b[0m\u001b[100;90m⠂⠂⠂⠂⠂⠂⠂⠂⠂\u001b[0m) ⠴ idealTree: \u001b[32;40mtiming\u001b[0m \u001b[35midealTree\u001b[0m Completed in 11ms\u001b[0m\u001b[K[0m\u001b[K\n",
      "up to date, audited 3 packages in 423ms\n",
      "\n",
      "found \u001b[32m\u001b[1m0\u001b[22m\u001b[39m vulnerabilities\n",
      "## NPM dependencies installed ##\n",
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with 20000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-2-count.jsonl\n",
      "Generated JSONL file with 20000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-5-count.jsonl\n",
      "Generated JSONL file with 20000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-10-count.jsonl\n",
      "Generated JSONL file with 20000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-20-count.jsonl\n",
      "Generated JSONL file with 8000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-100-count.jsonl\n",
      "Generated JSONL file with 5500 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-200-count.jsonl\n",
      "Generated JSONL file with 20000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-40-count.jsonl\n",
      "Generated JSONL file with 5000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-300-count.jsonl\n",
      "Generated JSONL file with 4500 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-400-count.jsonl\n",
      "Generated JSONL file with 2000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-900-count.jsonl\n",
      "Generated JSONL file with 2500 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-800-count.jsonl\n",
      "Generated JSONL file with 3500 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-600-count.jsonl\n",
      "Generated JSONL file with 20000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-80-count.jsonl\n",
      "Generated JSONL file with 4000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-500-count.jsonl\n",
      "Generated JSONL file with 3000 samples at /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory/memory_dataset/dataset/word-700-count.jsonl\n",
      "## Done ##\n"
     ]
    }
   ],
   "source": [
    "# Ensure NPM dependencies are installed\n",
    "!cd memory_dataset && npm install . && echo \"## NPM dependencies installed ##\"\n",
    "\n",
    "# Build the dataset\n",
    "!cd memory_dataset && ./gen_dataset_first_2k.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_2_offload\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/notebook/experiment/memory\n",
      "TRAINER_DIR: /home/picocreator/rwkv-proj/picocreator-memory-experiment/RWKV-v4neo\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 : Simple Memory finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/picocreator/.cache/huggingface/datasets/json/default-552a348358a7dd4c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6533.18it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 252.50it/s]\n",
      "Dataset json downloaded and prepared to /home/picocreator/.cache/huggingface/datasets/json/default-552a348358a7dd4c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.41it/s]\n",
      "mkdir: invalid option -- 'f'                                                    \n",
      "Try 'mkdir --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (enwiki_100k)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-finetune.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-mem-finetune/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-10 01:54:13,137] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-jit' with torch '2.0.1'\n",
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 1148776073\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 1148776073\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_015415-jf9erm5p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEcho-A-1B5 - Mem-Finetune-1 (bs=128, train-ctx=2048)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/jf9erm5p\u001b[0m\n",
      "Using /home/picocreator/.cache/torch_extensions/py311_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/picocreator/.cache/torch_extensions/py311_cu117/wkv_2048_bf16/build.ninja...\n",
      "Building extension module wkv_2048_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    }
   ],
   "source": [
    "# Start the foundation model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-finetune.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Finetune-1 (bs=128, train-ctx=2048)\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    && python export_checkpoint.py \"../checkpoint/Echo-A-1B5-mem-finetune/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets move and save this model\n",
    "!cd \"{TRAINER_DIR}\" && cp ./checkpoint/Echo-A-1B5-mem-finetune/last.ckpt/rwkv_model.pth ./model/Echo-A-1B5-Stage3.pth\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ./model/Echo-A-1B5-Stage3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && python3 dragon_test.py ../model/Echo-A-1B5-Stage3.pth \"cuda fp32\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
