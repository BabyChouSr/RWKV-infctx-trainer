{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-A 1B5 (Memory model from scratch)\n",
    "This attempts to build the memory model in stages, from scratch\n",
    "(Instead of previous attempts in doing foundation + gpt4all + etc)\n",
    "\n",
    "> This project assumes you have the rwkv-infctx conda env setup, and you are executing in that environment - see the main README.md for the conda env setup steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "Prepare and preload the finetuning process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Generating word reptition dataset ##\n",
      "Generated JSONL file with - 2 max words, 20000 samples - at ./dataset/word-2-count.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated JSONL file with - 5 max words, 20000 samples - at ./dataset/word-5-count.jsonl\n",
      "Generated JSONL file with - 10 max words, 20000 samples - at ./dataset/word-10-count.jsonl\n",
      "Generated JSONL file with - 20 max words, 20000 samples - at ./dataset/word-20-count.jsonl\n",
      "Generated JSONL file with - 100 max words, 10000 samples - at ./dataset/word-100-count.jsonl\n",
      "Generated JSONL file with - 40 max words, 20000 samples - at ./dataset/word-40-count.jsonl\n",
      "Generated JSONL file with - 80 max words, 20000 samples - at ./dataset/word-80-count.jsonl\n",
      "Generated JSONL file with - 200 max words, 10000 samples - at ./dataset/word-200-count.jsonl\n",
      "## Done ##\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "# Reset the dataset dir\n",
    "mkdir -p ./dataset\n",
    "rm -rf ./dataset/*.jsonl\n",
    "\n",
    "# Generate the various datasets\n",
    "echo \"## Generating word reptition dataset ##\"\n",
    "\n",
    "# We do a strong bias for smaller word count, to teach the concept from scratch\n",
    "# so that the model can learn the function\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-2-count.jsonl  2  20000 &\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-5-count.jsonl  5  20000 &\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-10-count.jsonl 10 20000 &\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-20-count.jsonl 20 20000 &\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-40-count.jsonl 40 20000 &\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-80-count.jsonl 80 20000 &\n",
    "\n",
    "# With a slight mix of the larger word count\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-100-count.jsonl 100 10000 &\n",
    "python ./memory_script/gen_dataset_file.py ./dataset/word-200-count.jsonl 200 10000 &\n",
    "\n",
    "wait\n",
    "echo \"## Done ##\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPSPEED_STRAT: deepspeed_stage_2_offload\n",
      "ENABLE_WANDB: True\n",
      "GPU_DEVICES: auto\n",
      "NOTEBOOK_DIR: /home/ubuntu/picocreator-memory-experiment/notebook/experiment/memory\n",
      "TRAINER_DIR: /home/ubuntu/picocreator-memory-experiment/RWKV-v4neo\n"
     ]
    }
   ],
   "source": [
    "DEEPSPEED_STRAT=\"deepspeed_stage_2_offload\"\n",
    "GPU_DEVICES=\"auto\"\n",
    "ENABLE_WANDB=True\n",
    "WANDB_PREFIX=\"Echo-A-1B5\"\n",
    "\n",
    "print(\"DEEPSPEED_STRAT:\", DEEPSPEED_STRAT)\n",
    "print(\"ENABLE_WANDB:\", ENABLE_WANDB)\n",
    "print(\"GPU_DEVICES:\", GPU_DEVICES)\n",
    "\n",
    "if ENABLE_WANDB:\n",
    "    WANDB_MODE=\"online\"\n",
    "else:\n",
    "    WANDB_MODE=\"disabled\"\n",
    "\n",
    "# Computing the notebook, and various paths\n",
    "import os\n",
    "NOTEBOOK_DIR=os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "TRAINER_DIR=os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../../../RWKV-v4neo/\"))\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"TRAINER_DIR:\", TRAINER_DIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Low word count memory training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 6921.29it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 462.85it/s]\n",
      "Dataset json downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 97.31it/s]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lets preload the requried dataset (<=80 words focused + <= some 900 words mixed in)\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    python3 preload_dataset.py \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-scratch.yaml\"\n",
    "\n",
    "# Ensure the checkpoint directory exists\n",
    "!cd \"{TRAINER_DIR}\" && mkdir -p \"../checkpoint/Echo-A-1B5-mem-scratch/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-10 11:49:04,657] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[RWKV.model] Running RWKV model using 'torch-compile' with torch '2.1.0.dev20230706'\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/utilities/seed.py:39: UserWarning: No seed found, seed set to 639376500\n",
      "  rank_zero_warn(f\"No seed found, seed set to {seed}\")\n",
      "Global seed set to 639376500\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicocreator\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20230710_114906-ijl8t43q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mEcho-A-1B5 - Mem-Train-Stage-1 (bs=32, train-ctx=1024)\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/ijl8t43q\u001b[0m\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/wkv_1024_bf16/build.ninja...\n",
      "Building extension module wkv_1024_bf16...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -t 4 -std=c++17 -res-usage --maxrregcount 60 --use_fast_math -O3 -Xptxas -O3 --extra-device-vectorization -DTmax=1024 -c /home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_cuda_bf16.cu -o wkv_cuda_bf16.cuda.o \n",
      "ptxas info    : 1 bytes gmem\n",
      "ptxas info    : Compiling entry function '_Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z15kernel_backwardiiiPKfPKN3c108BFloat16ES4_S4_S0_S4_S0_PS2_S5_S5_S5_Pf\n",
      "    12288 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 56 registers, 464 bytes cmem[0], 8 bytes cmem[2]\n",
      "ptxas info    : Compiling entry function '_Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf' for 'sm_86'\n",
      "ptxas info    : Function properties for _Z14kernel_forwardiiiPKfPKN3c108BFloat16ES4_S4_S0_PS2_Pf\n",
      "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
      "ptxas info    : Used 40 registers, 424 bytes cmem[0]\n",
      "[2/3] c++ -MMD -MF wkv_op_bf16.o.d -DTORCH_EXTENSION_NAME=wkv_1024_bf16 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/TH -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ubuntu/anaconda3/envs/rwkv-infctx/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -std=c++17 -O3 -DTmax=1024 -c /home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/cuda/wkv_op_bf16.cpp -o wkv_op_bf16.o \n",
      "[3/3] c++ wkv_op_bf16.o wkv_cuda_bf16.cuda.o -shared -L/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o wkv_1024_bf16.so\n",
      "Loading extension module wkv_1024_bf16...\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/fabric/connector.py:555: UserWarning: bf16 is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 320.10it/s]\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-4d0d02db630cf0a0_*_of_00032.arrow\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/json/default-027bf84f8fbbf3d0/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c6cc97c105981bb4_*_of_00032.arrow\n",
      "[rank: 0] Global seed set to 639376500                                          \n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[2023-07-10 11:49:53,314] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
      "[2023-07-10 11:49:53,318] [WARNING] [deepspeed.py:638:_auto_select_batch_size] Tried to infer the batch size for internal deepspeed logging from the `train_dataloader()`. To ensure DeepSpeed logging remains correct, please manually pass the plugin with the batch size, `Trainer(strategy=DeepSpeedStrategy(logging_batch_size_per_gpu=batch_size))`.\n",
      "Enabling DeepSpeed BF16.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using /home/ubuntu/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.3406479358673096 seconds\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Rank: 0 partition count [1, 1, 1] and sizes[(1515008000, False), (49152, False), (49152, False)] \n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | emb    | Embedding  | 102 M \n",
      "1 | blocks | ModuleList | 1.3 B \n",
      "2 | ln_out | LayerNorm  | 4.1 K \n",
      "3 | head   | Linear     | 102 M \n",
      "--------------------------------------\n",
      "1.5 B     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 B     Total params\n",
      "6,060.425 Total estimated model params size (MB)\n",
      "Epoch 0:   0%| | 1/133000 [01:06<2441:07:41, 66.08s/it, v_num=t43q, train/loss=1/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/overrides.py:112: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/overrides.py:119: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "[2023-07-10 11:51:40,849] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: '_forward_kvsr' (/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py:231)\n",
      "to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/new_train.py\", line 15, in <module>\n",
      "    cli_main()\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/new_train.py\", line 8, in cli_main\n",
      "    LightningCLI(\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 353, in __init__\n",
      "    self._run_subcommand(self.subcommand)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/cli.py\", line 642, in _run_subcommand\n",
      "    fn(**fn_kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 41, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 91, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1018, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 201, in run\n",
      "    self.advance()\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 354, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 133, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 218, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 185, in run\n",
      "    self._optimizer_step(kwargs.get(\"batch_idx\", 0), closure)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 260, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 140, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1256, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 155, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 256, in optimizer_step\n",
      "    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 225, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 92, in optimizer_step\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 126, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 307, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 287, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 328, in training_step\n",
      "    return self.model(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
      "    ret_val = func(*args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1735, in forward\n",
      "    loss = self.module(*inputs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/lightning/pytorch/overrides/base.py\", line 90, in forward\n",
      "    output = self._forward_module.training_step(*inputs, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 980, in training_step\n",
      "    total_loss = self.compute_loss(batch, batch_idx, True)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 894, in compute_loss\n",
      "    segment_loss, new_shift_states, new_wkv_states, steps = checkpointed_step(\n",
      "                                                            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 776, in checkpointed_step\n",
      "    logits, new_shift_states, new_wkv_states = self(\n",
      "                                               ^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 670, in forward\n",
      "    x, new_state = deepspeed_checkpoint(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 294, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 123, in deepspeed_checkpoint\n",
      "    return deepspeed.checkpointing.checkpoint(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 713, in checkpoint\n",
      "    CheckpointFunction.apply(function, all_outputs, *args)\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py\", line 555, in forward\n",
      "    outputs = run_function(*inputs_cuda)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 319, in forward\n",
      "    att_out, att_state = self.att(\n",
      "                         ^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1505, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1514, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/picocreator-memory-experiment/RWKV-v4neo/src/model.py\", line 257, in forward\n",
      "    k, v, sr = self._forward_kvsr(x, last_state)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 294, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 447, in catch_errors\n",
      "    return callback(frame, cache_size, hooks, frame_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 127, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 336, in _convert_frame_assert\n",
      "    unimplemented(\"cache_size_limit reached\")\n",
      "  File \"/home/ubuntu/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/torch/_dynamo/exc.py\", line 141, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: cache_size_limit reached\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              batchidx 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           global_rank 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          real_ctx_len 121\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               substep 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss 10.8125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   trainer/global_step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: trainer/learning_rate 0.0008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mEcho-A-1B5 - Mem-Train-Stage-1 (bs=32, train-ctx=1024)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/picocreator/RWKV-Memory-Experiment/runs/ijl8t43q\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230710_114906-ijl8t43q/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Start the memory model training\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    export RWKV_TORCH_COMPILE=0 && \\\n",
    "    export RWKV_JIT_ON=1 && \\\n",
    "    python new_train.py fit \\\n",
    "        -c \"{NOTEBOOK_DIR}/Echo-A-1B5-mem-scratch.yaml\" \\\n",
    "        --trainer.logger.init_args.name=\"{WANDB_PREFIX} - Mem-Train-Stage-1 (bs=64, train-ctx=1024)\" \\\n",
    "        --trainer.strategy=\"{DEEPSPEED_STRAT}\" \\\n",
    "        --trainer.devices=\"{GPU_DEVICES}\"  \\\n",
    "        --model.ctx_len=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets export the model from the checkpoint\n",
    "!cd \"{TRAINER_DIR}\" && \\\n",
    "    && python export_checkpoint.py \"../checkpoint/Echo-A-1B5-mem-scratch/last.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets move and save this model\n",
    "!cd \"{TRAINER_DIR}\" && cp ./checkpoint/Echo-A-1B5-mem-scratch/last.ckpt/rwkv_model.pth ./model/Echo-A-1B5-Tune-1.pth\n",
    "!cd \"{TRAINER_DIR}\" && ls -alh ./model/Echo-A-1B5-Tune-1.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a quick dragon prompt validation\n",
    "!cd \"{TRAINER_DIR}\" && python3 dragon_test.py ../model/Echo-A-1B5-Tune-1.pth \"cuda fp32\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
